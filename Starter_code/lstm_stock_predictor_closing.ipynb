{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Stock Predictor Using Closing Prices\n",
    "\n",
    "In this notebook, you will build and train a custom LSTM RNN that uses a 10 day window of Bitcoin closing prices to predict the 11th day closing price. \n",
    "\n",
    "You will need to:\n",
    "\n",
    "1. Prepare the data for training and testing\n",
    "2. Build and train a custom LSTM RNN\n",
    "3. Evaluate the performance of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "In this section, you will need to prepare the training and testing data for the model. The model will use a rolling 10 day window to predict the 11th day closing price.\n",
    "\n",
    "You will need to:\n",
    "1. Use the `window_data` function to generate the X and y values for the model.\n",
    "2. Split the data into 70% training and 30% testing\n",
    "3. Apply the MinMaxScaler to the X and y values\n",
    "4. Reshape the X_train and X_test data for the model. Note: The required input format for the LSTM is:\n",
    "\n",
    "```python\n",
    "reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hvplot.pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed for reproducibility\n",
    "# Note: This is for the homework solution, but it is good practice to comment this out and run multiple experiments to evaluate your model\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import random\n",
    "random.set_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fng_value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-07-29</th>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-28</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-27</th>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-26</th>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-25</th>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             fng_value\n",
       "date                  \n",
       "2019-07-29          19\n",
       "2019-07-28          16\n",
       "2019-07-27          47\n",
       "2019-07-26          24\n",
       "2019-07-25          42"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the fear and greed sentiment data for Bitcoin\n",
    "df = pd.read_csv('btc_sentiment.csv', index_col=\"date\", infer_datetime_format=True, parse_dates=True)\n",
    "df = df.drop(columns=\"fng_classification\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date\n",
       "2019-07-25    9882.429688\n",
       "2019-07-26    9847.450195\n",
       "2019-07-27    9478.320313\n",
       "2019-07-28    9531.769531\n",
       "2019-07-29    9529.889648\n",
       "Name: Close, dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the historical closing prices for Bitcoin\n",
    "df2 = pd.read_csv('btc_historic.csv', index_col=\"Date\", infer_datetime_format=True, parse_dates=True)['Close']\n",
    "df2 = df2.sort_index()\n",
    "df2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fng_value</th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-07-25</th>\n",
       "      <td>42</td>\n",
       "      <td>9882.429688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-26</th>\n",
       "      <td>24</td>\n",
       "      <td>9847.450195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-27</th>\n",
       "      <td>47</td>\n",
       "      <td>9478.320313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-28</th>\n",
       "      <td>16</td>\n",
       "      <td>9531.769531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-29</th>\n",
       "      <td>19</td>\n",
       "      <td>9529.889648</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             fng_value        Close\n",
       "2019-07-25          42  9882.429688\n",
       "2019-07-26          24  9847.450195\n",
       "2019-07-27          47  9478.320313\n",
       "2019-07-28          16  9531.769531\n",
       "2019-07-29          19  9529.889648"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Join the data into a single DataFrame\n",
    "df = df.join(df2, how=\"inner\")\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fng_value</th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-02-01</th>\n",
       "      <td>30</td>\n",
       "      <td>9114.719727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-02</th>\n",
       "      <td>15</td>\n",
       "      <td>8870.820313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-03</th>\n",
       "      <td>40</td>\n",
       "      <td>9251.269531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-04</th>\n",
       "      <td>24</td>\n",
       "      <td>8218.049805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-05</th>\n",
       "      <td>11</td>\n",
       "      <td>6937.080078</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             fng_value        Close\n",
       "2018-02-01          30  9114.719727\n",
       "2018-02-02          15  8870.820313\n",
       "2018-02-03          40  9251.269531\n",
       "2018-02-04          24  8218.049805\n",
       "2018-02-05          11  6937.080078"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function accepts the column number for the features (X) and the target (y)\n",
    "# It chunks the data up with a rolling window of Xt-n to predict Xt\n",
    "# It returns a numpy array of X any y\n",
    "def window_data(df, window, feature_col_number, target_col_number):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(len(df) - window - 1):\n",
    "        features = df.iloc[i:(i + window), feature_col_number]\n",
    "        target = df.iloc[(i + window), target_col_number]\n",
    "        X.append(features)\n",
    "        y.append(target)\n",
    "    return np.array(X), np.array(y).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict Closing Prices using a 10 day window of previous closing prices\n",
    "# Then, experiment with window sizes anywhere from 1 to 10 and see how the model performance changes\n",
    "window_size = 10\n",
    "\n",
    "# Column index 0 is the 'fng_value' column\n",
    "# Column index 1 is the `Close` column\n",
    "feature_column = 1\n",
    "target_column = 1\n",
    "X, y = window_data(df, window_size, feature_column, target_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 70% of the data for training and the remaineder for testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Use the MinMaxScaler to scale data between 0 and 1.\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaler.fit(X)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "scaler.fit(y)\n",
    "y_train = scaler.transform(y_train)\n",
    "y_test = scaler.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the features for the model\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Train the LSTM RNN\n",
    "\n",
    "In this section, you will design a custom LSTM RNN and fit (train) it using the training data.\n",
    "\n",
    "You will need to:\n",
    "1. Define the model architecture\n",
    "2. Compile the model\n",
    "3. Fit the model to the training data\n",
    "\n",
    "### Hints:\n",
    "You will want to use the same model architecture and random seed for both notebooks. This is necessary to accurately compare the performance of the FNG model vs the closing price model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the LSTM model. \n",
    "# The return sequences need to be set to True if you are adding additional LSTM layers, but \n",
    "# You don't have to do this for the final layer. \n",
    "# Note: The dropouts help prevent overfitting\n",
    "# Note: The input shape is the number of time steps and the number of indicators\n",
    "# Note: Batching inputs has a different input shape of Samples/TimeSteps/Features\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "dropout_fraction = 0.2\n",
    "\n",
    "# Layer 1\n",
    "model.add(LSTM(\n",
    "    units = 100,\n",
    "    return_sequences = True,\n",
    "    input_shape = (X_train.shape[1],1))\n",
    "         )\n",
    "model.add(Dropout(dropout_fraction))\n",
    "\n",
    "# Layer 2\n",
    "model.add(LSTM(\n",
    "    units = 50,\n",
    "    return_sequences = True,\n",
    "        ))\n",
    "model.add(Dropout(dropout_fraction))\n",
    "\n",
    "# Layer 3\n",
    "model.add(LSTM(\n",
    "    units = 40,\n",
    "    return_sequences = False,\n",
    "        ))\n",
    "model.add(Dropout(dropout_fraction))\n",
    "\n",
    "model.add(Dense(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_3 (LSTM)               (None, 10, 100)           40800     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 10, 100)           0         \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 10, 50)            30200     \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 10, 50)            0         \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 40)                14560     \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 40)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 41        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 85,601\n",
      "Trainable params: 85,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Summarize the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "12/12 [==============================] - 6s 18ms/step - loss: 0.0665\n",
      "Epoch 2/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0176\n",
      "Epoch 3/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0119\n",
      "Epoch 4/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0077\n",
      "Epoch 5/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0075\n",
      "Epoch 6/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0064\n",
      "Epoch 7/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0070\n",
      "Epoch 8/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0065\n",
      "Epoch 9/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0068\n",
      "Epoch 10/250\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.0068\n",
      "Epoch 11/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0073\n",
      "Epoch 12/250\n",
      "12/12 [==============================] - 0s 22ms/step - loss: 0.0068\n",
      "Epoch 13/250\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.0057\n",
      "Epoch 14/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0056\n",
      "Epoch 15/250\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.0058\n",
      "Epoch 16/250\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.0061\n",
      "Epoch 17/250\n",
      "12/12 [==============================] - 0s 22ms/step - loss: 0.0063\n",
      "Epoch 18/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0059\n",
      "Epoch 19/250\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0062\n",
      "Epoch 20/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0059\n",
      "Epoch 21/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0057\n",
      "Epoch 22/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0050\n",
      "Epoch 23/250\n",
      "12/12 [==============================] - 0s 21ms/step - loss: 0.0057\n",
      "Epoch 24/250\n",
      "12/12 [==============================] - 0s 21ms/step - loss: 0.0054\n",
      "Epoch 25/250\n",
      "12/12 [==============================] - 0s 21ms/step - loss: 0.0051\n",
      "Epoch 26/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0054\n",
      "Epoch 27/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0053\n",
      "Epoch 28/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0052\n",
      "Epoch 29/250\n",
      "12/12 [==============================] - 0s 21ms/step - loss: 0.0046\n",
      "Epoch 30/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0050\n",
      "Epoch 31/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0045\n",
      "Epoch 32/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0052\n",
      "Epoch 33/250\n",
      "12/12 [==============================] - 0s 20ms/step - loss: 0.0046\n",
      "Epoch 34/250\n",
      "12/12 [==============================] - 0s 22ms/step - loss: 0.0047\n",
      "Epoch 35/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0047\n",
      "Epoch 36/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0052\n",
      "Epoch 37/250\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0038\n",
      "Epoch 38/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0048\n",
      "Epoch 39/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0046\n",
      "Epoch 40/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0047\n",
      "Epoch 41/250\n",
      "12/12 [==============================] - 0s 30ms/step - loss: 0.0053\n",
      "Epoch 42/250\n",
      "12/12 [==============================] - 0s 20ms/step - loss: 0.0044\n",
      "Epoch 43/250\n",
      "12/12 [==============================] - 0s 21ms/step - loss: 0.0051\n",
      "Epoch 44/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0045\n",
      "Epoch 45/250\n",
      "12/12 [==============================] - 0s 28ms/step - loss: 0.0043\n",
      "Epoch 46/250\n",
      "12/12 [==============================] - 0s 20ms/step - loss: 0.0046\n",
      "Epoch 47/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0046\n",
      "Epoch 48/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0042\n",
      "Epoch 49/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0033\n",
      "Epoch 50/250\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0035\n",
      "Epoch 51/250\n",
      "12/12 [==============================] - 0s 20ms/step - loss: 0.0043\n",
      "Epoch 52/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0040\n",
      "Epoch 53/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0033\n",
      "Epoch 54/250\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0037\n",
      "Epoch 55/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0034\n",
      "Epoch 56/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0038\n",
      "Epoch 57/250\n",
      "12/12 [==============================] - 0s 20ms/step - loss: 0.0039\n",
      "Epoch 58/250\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.0032\n",
      "Epoch 59/250\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0036\n",
      "Epoch 60/250\n",
      "12/12 [==============================] - 0s 21ms/step - loss: 0.0036\n",
      "Epoch 61/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0031\n",
      "Epoch 62/250\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0035\n",
      "Epoch 63/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0031\n",
      "Epoch 64/250\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.0033\n",
      "Epoch 65/250\n",
      "12/12 [==============================] - 0s 22ms/step - loss: 0.0030\n",
      "Epoch 66/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0032\n",
      "Epoch 67/250\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0032\n",
      "Epoch 68/250\n",
      "12/12 [==============================] - 0s 22ms/step - loss: 0.0034\n",
      "Epoch 69/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0037\n",
      "Epoch 70/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0034\n",
      "Epoch 71/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0034\n",
      "Epoch 72/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0032\n",
      "Epoch 73/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0035\n",
      "Epoch 74/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0033\n",
      "Epoch 75/250\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0026\n",
      "Epoch 76/250\n",
      "12/12 [==============================] - 0s 22ms/step - loss: 0.0030\n",
      "Epoch 77/250\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0036\n",
      "Epoch 78/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0033\n",
      "Epoch 79/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0033\n",
      "Epoch 80/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0029\n",
      "Epoch 81/250\n",
      "12/12 [==============================] - 0s 20ms/step - loss: 0.0030\n",
      "Epoch 82/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0031\n",
      "Epoch 83/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0033\n",
      "Epoch 84/250\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0026\n",
      "Epoch 85/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0028\n",
      "Epoch 86/250\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.0033\n",
      "Epoch 87/250\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.0025\n",
      "Epoch 88/250\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.0028\n",
      "Epoch 89/250\n",
      "12/12 [==============================] - 0s 20ms/step - loss: 0.0023\n",
      "Epoch 90/250\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0030\n",
      "Epoch 91/250\n",
      "12/12 [==============================] - 0s 20ms/step - loss: 0.0028\n",
      "Epoch 92/250\n",
      "12/12 [==============================] - 0s 20ms/step - loss: 0.0024\n",
      "Epoch 93/250\n",
      "12/12 [==============================] - 0s 21ms/step - loss: 0.0025\n",
      "Epoch 94/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0031\n",
      "Epoch 95/250\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0030\n",
      "Epoch 96/250\n",
      "12/12 [==============================] - 0s 20ms/step - loss: 0.0028\n",
      "Epoch 97/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0029\n",
      "Epoch 98/250\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0027\n",
      "Epoch 99/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0028\n",
      "Epoch 100/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0025\n",
      "Epoch 101/250\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0026\n",
      "Epoch 102/250\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0024\n",
      "Epoch 103/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0026\n",
      "Epoch 104/250\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.0023\n",
      "Epoch 105/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0021\n",
      "Epoch 106/250\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0022\n",
      "Epoch 107/250\n",
      "12/12 [==============================] - 0s 20ms/step - loss: 0.0023\n",
      "Epoch 108/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0024\n",
      "Epoch 109/250\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0025\n",
      "Epoch 110/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0021\n",
      "Epoch 111/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0020\n",
      "Epoch 112/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0022\n",
      "Epoch 113/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0026\n",
      "Epoch 114/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0029\n",
      "Epoch 115/250\n",
      "12/12 [==============================] - 0s 25ms/step - loss: 0.0024\n",
      "Epoch 116/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0029\n",
      "Epoch 117/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0028\n",
      "Epoch 118/250\n",
      "12/12 [==============================] - 0s 22ms/step - loss: 0.0022\n",
      "Epoch 119/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0023\n",
      "Epoch 120/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0023\n",
      "Epoch 121/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0024\n",
      "Epoch 122/250\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0020\n",
      "Epoch 123/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0020\n",
      "Epoch 124/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0024\n",
      "Epoch 125/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0023\n",
      "Epoch 126/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0025\n",
      "Epoch 127/250\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0022\n",
      "Epoch 128/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0021\n",
      "Epoch 129/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0025\n",
      "Epoch 130/250\n",
      "12/12 [==============================] - 0s 22ms/step - loss: 0.0020\n",
      "Epoch 131/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0024\n",
      "Epoch 132/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0024\n",
      "Epoch 133/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0022\n",
      "Epoch 134/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0021\n",
      "Epoch 135/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0024\n",
      "Epoch 136/250\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0025\n",
      "Epoch 137/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0024\n",
      "Epoch 138/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0024\n",
      "Epoch 139/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0021\n",
      "Epoch 140/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0023\n",
      "Epoch 141/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0024\n",
      "Epoch 142/250\n",
      "12/12 [==============================] - 0s 21ms/step - loss: 0.0022\n",
      "Epoch 143/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0019\n",
      "Epoch 144/250\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0020\n",
      "Epoch 145/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0024\n",
      "Epoch 146/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0023\n",
      "Epoch 147/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0021\n",
      "Epoch 148/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0020\n",
      "Epoch 149/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0021\n",
      "Epoch 150/250\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0023\n",
      "Epoch 151/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0020\n",
      "Epoch 152/250\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0019\n",
      "Epoch 153/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0020\n",
      "Epoch 154/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0021\n",
      "Epoch 155/250\n",
      "12/12 [==============================] - 0s 22ms/step - loss: 0.0024\n",
      "Epoch 156/250\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.0026\n",
      "Epoch 157/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0022\n",
      "Epoch 158/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0022\n",
      "Epoch 159/250\n",
      "12/12 [==============================] - 0s 20ms/step - loss: 0.0024\n",
      "Epoch 160/250\n",
      "12/12 [==============================] - 0s 21ms/step - loss: 0.0018\n",
      "Epoch 161/250\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0023\n",
      "Epoch 162/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0024\n",
      "Epoch 163/250\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0025\n",
      "Epoch 164/250\n",
      "12/12 [==============================] - 0s 21ms/step - loss: 0.0024\n",
      "Epoch 165/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0024\n",
      "Epoch 166/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0023\n",
      "Epoch 167/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0020\n",
      "Epoch 168/250\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0022\n",
      "Epoch 169/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0022\n",
      "Epoch 170/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0024\n",
      "Epoch 171/250\n",
      "12/12 [==============================] - 0s 20ms/step - loss: 0.0019\n",
      "Epoch 172/250\n",
      "12/12 [==============================] - 0s 21ms/step - loss: 0.0017\n",
      "Epoch 173/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0022\n",
      "Epoch 174/250\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.0016\n",
      "Epoch 175/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0023\n",
      "Epoch 176/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0019\n",
      "Epoch 177/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0018\n",
      "Epoch 178/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0019\n",
      "Epoch 179/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0019\n",
      "Epoch 180/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0020\n",
      "Epoch 181/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0019\n",
      "Epoch 182/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0023\n",
      "Epoch 183/250\n",
      "12/12 [==============================] - 0s 25ms/step - loss: 0.0021\n",
      "Epoch 184/250\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.0018\n",
      "Epoch 185/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0021\n",
      "Epoch 186/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0019\n",
      "Epoch 187/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0021\n",
      "Epoch 188/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0019\n",
      "Epoch 189/250\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.0019\n",
      "Epoch 190/250\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.0022\n",
      "Epoch 191/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0020\n",
      "Epoch 192/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0016\n",
      "Epoch 193/250\n",
      "12/12 [==============================] - 0s 21ms/step - loss: 0.0018\n",
      "Epoch 194/250\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.0021\n",
      "Epoch 195/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0018\n",
      "Epoch 196/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0024\n",
      "Epoch 197/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0019\n",
      "Epoch 198/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0019\n",
      "Epoch 199/250\n",
      "12/12 [==============================] - 0s 21ms/step - loss: 0.0020\n",
      "Epoch 200/250\n",
      "12/12 [==============================] - 0s 20ms/step - loss: 0.0020\n",
      "Epoch 201/250\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.0022\n",
      "Epoch 202/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0020\n",
      "Epoch 203/250\n",
      "12/12 [==============================] - 0s 21ms/step - loss: 0.0018\n",
      "Epoch 204/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0019\n",
      "Epoch 205/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0016\n",
      "Epoch 206/250\n",
      "12/12 [==============================] - 0s 22ms/step - loss: 0.0021\n",
      "Epoch 207/250\n",
      "12/12 [==============================] - 0s 21ms/step - loss: 0.0017\n",
      "Epoch 208/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0017\n",
      "Epoch 209/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0018\n",
      "Epoch 210/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0019\n",
      "Epoch 211/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0019\n",
      "Epoch 212/250\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0019\n",
      "Epoch 213/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0019\n",
      "Epoch 214/250\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.0017\n",
      "Epoch 215/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0020\n",
      "Epoch 216/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0020\n",
      "Epoch 217/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0019\n",
      "Epoch 218/250\n",
      "12/12 [==============================] - 0s 20ms/step - loss: 0.0023\n",
      "Epoch 219/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0019\n",
      "Epoch 220/250\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.0017\n",
      "Epoch 221/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0018\n",
      "Epoch 222/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0020\n",
      "Epoch 223/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0017\n",
      "Epoch 224/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0017\n",
      "Epoch 225/250\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0018\n",
      "Epoch 226/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0016\n",
      "Epoch 227/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0021\n",
      "Epoch 228/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0017\n",
      "Epoch 229/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0019\n",
      "Epoch 230/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0019\n",
      "Epoch 231/250\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0017\n",
      "Epoch 232/250\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.0018\n",
      "Epoch 233/250\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.0018\n",
      "Epoch 234/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0020\n",
      "Epoch 235/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0020\n",
      "Epoch 236/250\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.0020\n",
      "Epoch 237/250\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0019\n",
      "Epoch 238/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0019\n",
      "Epoch 239/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0018\n",
      "Epoch 240/250\n",
      "12/12 [==============================] - 0s 20ms/step - loss: 0.0018\n",
      "Epoch 241/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0018\n",
      "Epoch 242/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0019\n",
      "Epoch 243/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0019\n",
      "Epoch 244/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0020\n",
      "Epoch 245/250\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.0021\n",
      "Epoch 246/250\n",
      "12/12 [==============================] - 0s 20ms/step - loss: 0.0019\n",
      "Epoch 247/250\n",
      "12/12 [==============================] - 0s 22ms/step - loss: 0.0019\n",
      "Epoch 248/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0017\n",
      "Epoch 249/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0019\n",
      "Epoch 250/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0021\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x187c6c2c430>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "# Use at least 10 epochs\n",
    "# Do not shuffle the data\n",
    "# Experiement with the batch size, but a smaller batch size is recommended\n",
    "model.fit(X_train, y_train, epochs=250, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance\n",
    "\n",
    "In this section, you will evaluate the model using the test data. \n",
    "\n",
    "You will need to:\n",
    "1. Evaluate the model using the `X_test` and `y_test` data.\n",
    "2. Use the X_test data to make predictions\n",
    "3. Create a DataFrame of Real (y_test) vs predicted values. \n",
    "4. Plot the Real vs predicted values as a line chart\n",
    "\n",
    "### Hints\n",
    "Remember to apply the `inverse_transform` function to the predicted and y_test values to recover the actual closing prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 6ms/step - loss: 0.0016\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0016227567102760077"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make some predictions\n",
    "predicted = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recover the original prices instead of the scaled version\n",
    "predicted_prices = scaler.inverse_transform(predicted)\n",
    "real_prices = scaler.inverse_transform(y_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Real</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-02-20</th>\n",
       "      <td>7503.200195</td>\n",
       "      <td>7887.425781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-21</th>\n",
       "      <td>6583.049805</td>\n",
       "      <td>6623.910156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-22</th>\n",
       "      <td>5289.750000</td>\n",
       "      <td>5355.658203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-23</th>\n",
       "      <td>5234.089844</td>\n",
       "      <td>5510.272949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-24</th>\n",
       "      <td>6632.870117</td>\n",
       "      <td>6718.647949</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Real    Predicted\n",
       "2019-02-20  7503.200195  7887.425781\n",
       "2019-02-21  6583.049805  6623.910156\n",
       "2019-02-22  5289.750000  5355.658203\n",
       "2019-02-23  5234.089844  5510.272949\n",
       "2019-02-24  6632.870117  6718.647949"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DataFrame of Real and Predicted values\n",
    "stocks = pd.DataFrame({\n",
    "    \"Real\": real_prices.ravel(),\n",
    "    \"Predicted\": predicted_prices.ravel()\n",
    "}, index = df.index[-len(real_prices): ]) \n",
    "stocks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.holoviews_exec.v0+json": "",
      "text/html": [
       "<div id='1002'>\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "  <div class=\"bk-root\" id=\"b50d63d5-39fa-4712-89bf-f76fec7cb20a\" data-root-id=\"1002\"></div>\n",
       "</div>\n",
       "<script type=\"application/javascript\">(function(root) {\n",
       "  function embed_document(root) {\n",
       "    var docs_json = {\"92afe17d-448b-4b33-938b-3d474052f139\":{\"defs\":[{\"extends\":null,\"module\":null,\"name\":\"ReactiveHTML1\",\"overrides\":[],\"properties\":[]},{\"extends\":null,\"module\":null,\"name\":\"FlexBox1\",\"overrides\":[],\"properties\":[{\"default\":\"flex-start\",\"kind\":null,\"name\":\"align_content\"},{\"default\":\"flex-start\",\"kind\":null,\"name\":\"align_items\"},{\"default\":\"row\",\"kind\":null,\"name\":\"flex_direction\"},{\"default\":\"wrap\",\"kind\":null,\"name\":\"flex_wrap\"},{\"default\":\"flex-start\",\"kind\":null,\"name\":\"justify_content\"}]},{\"extends\":null,\"module\":null,\"name\":\"GridStack1\",\"overrides\":[],\"properties\":[{\"default\":\"warn\",\"kind\":null,\"name\":\"mode\"},{\"default\":null,\"kind\":null,\"name\":\"ncols\"},{\"default\":null,\"kind\":null,\"name\":\"nrows\"},{\"default\":true,\"kind\":null,\"name\":\"allow_resize\"},{\"default\":true,\"kind\":null,\"name\":\"allow_drag\"},{\"default\":[],\"kind\":null,\"name\":\"state\"}]},{\"extends\":null,\"module\":null,\"name\":\"click1\",\"overrides\":[],\"properties\":[{\"default\":\"\",\"kind\":null,\"name\":\"terminal_output\"},{\"default\":\"\",\"kind\":null,\"name\":\"debug_name\"},{\"default\":0,\"kind\":null,\"name\":\"clears\"}]},{\"extends\":null,\"module\":null,\"name\":\"NotificationAreaBase1\",\"overrides\":[],\"properties\":[{\"default\":\"bottom-right\",\"kind\":null,\"name\":\"position\"},{\"default\":0,\"kind\":null,\"name\":\"_clear\"}]},{\"extends\":null,\"module\":null,\"name\":\"NotificationArea1\",\"overrides\":[],\"properties\":[{\"default\":[],\"kind\":null,\"name\":\"notifications\"},{\"default\":\"bottom-right\",\"kind\":null,\"name\":\"position\"},{\"default\":0,\"kind\":null,\"name\":\"_clear\"},{\"default\":[{\"background\":\"#ffc107\",\"icon\":{\"className\":\"fas fa-exclamation-triangle\",\"color\":\"white\",\"tagName\":\"i\"},\"type\":\"warning\"},{\"background\":\"#007bff\",\"icon\":{\"className\":\"fas fa-info-circle\",\"color\":\"white\",\"tagName\":\"i\"},\"type\":\"info\"}],\"kind\":null,\"name\":\"types\"}]},{\"extends\":null,\"module\":null,\"name\":\"Notification\",\"overrides\":[],\"properties\":[{\"default\":null,\"kind\":null,\"name\":\"background\"},{\"default\":3000,\"kind\":null,\"name\":\"duration\"},{\"default\":null,\"kind\":null,\"name\":\"icon\"},{\"default\":\"\",\"kind\":null,\"name\":\"message\"},{\"default\":null,\"kind\":null,\"name\":\"notification_type\"},{\"default\":false,\"kind\":null,\"name\":\"_destroyed\"}]},{\"extends\":null,\"module\":null,\"name\":\"TemplateActions1\",\"overrides\":[],\"properties\":[{\"default\":0,\"kind\":null,\"name\":\"open_modal\"},{\"default\":0,\"kind\":null,\"name\":\"close_modal\"}]},{\"extends\":null,\"module\":null,\"name\":\"MaterialTemplateActions1\",\"overrides\":[],\"properties\":[{\"default\":0,\"kind\":null,\"name\":\"open_modal\"},{\"default\":0,\"kind\":null,\"name\":\"close_modal\"}]}],\"roots\":{\"references\":[{\"attributes\":{},\"id\":\"1045\",\"type\":\"Selection\"},{\"attributes\":{\"click_policy\":\"mute\",\"coordinates\":null,\"group\":null,\"items\":[{\"id\":\"1075\"},{\"id\":\"1109\"}],\"location\":[0,0],\"title\":\"Variable\"},\"id\":\"1074\",\"type\":\"Legend\"},{\"attributes\":{},\"id\":\"1026\",\"type\":\"PanTool\"},{\"attributes\":{\"margin\":[5,5,5,5],\"name\":\"HSpacer01706\",\"sizing_mode\":\"stretch_width\"},\"id\":\"1303\",\"type\":\"Spacer\"},{\"attributes\":{\"axis_label\":\"Date\",\"coordinates\":null,\"formatter\":{\"id\":\"1039\"},\"group\":null,\"major_label_policy\":{\"id\":\"1040\"},\"ticker\":{\"id\":\"1018\"}},\"id\":\"1017\",\"type\":\"DatetimeAxis\"},{\"attributes\":{\"data\":{\"Variable\":[\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\"],\"index\":{\"__ndarray__\":\"AABAO4OQdkIAAACh1ZB2QgAAwAYokXZCAACAbHqRdkIAAEDSzJF2QgAAADgfknZCAADAnXGSdkIAAIADxJJ2QgAAQGkWk3ZCAAAAz2iTdkIAAMA0u5N2QgAAgJoNlHZCAABAAGCUdkIAAABmspR2QgAAwMsElXZCAACAMVeVdkIAAECXqZV2QgAAAP37lXZCAADAYk6WdkIAAIDIoJZ2QgAAQC7zlnZCAAAAlEWXdkIAAMD5l5d2QgAAgF/ql3ZCAABAxTyYdkIAAAArj5h2QgAAwJDhmHZCAACA9jOZdkIAAEBchpl2QgAAAMLYmXZCAADAJyuadkIAAICNfZp2QgAAQPPPmnZCAAAAWSKbdkIAAMC+dJt2QgAAgCTHm3ZCAABAihmcdkIAAADwa5x2QgAAwFW+nHZCAACAuxCddkIAAEAhY512QgAAAIe1nXZCAADA7AeedkIAAIBSWp52QgAAQLisnnZCAAAAHv+edkIAAMCDUZ92QgAAgOmjn3ZCAABAT/afdkIAAAC1SKB2QgAAwBqboHZCAACAgO2gdkIAAEDmP6F2QgAAAEySoXZCAADAseShdkIAAIAXN6J2QgAAQH2JonZCAAAA49uidkIAAMBILqN2QgAAgK6Ao3ZCAABAFNOjdkIAAAB6JaR2QgAAwN93pHZCAACARcqkdkIAAECrHKV2QgAAABFvpXZCAADAdsGldkIAAIDcE6Z2QgAAQEJmpnZCAAAAqLimdkIAAMANC6d2QgAAgHNdp3ZCAABA2a+ndkIAAAA/Aqh2QgAAwKRUqHZCAACACqeodkIAAEBw+ah2QgAAANZLqXZCAADAO56pdkIAAICh8Kl2QgAAQAdDqnZCAAAAbZWqdkIAAMDS56p2QgAAgDg6q3ZCAABAnoyrdkIAAAAE36t2QgAAwGkxrHZCAACAz4OsdkIAAEA11qx2QgAAAJsorXZCAADAAHutdkIAAIBmza12QgAAQMwfrnZCAAAAMnKudkIAAMCXxK52QgAAgP0Wr3ZCAABAY2mvdkIAAADJu692QgAAwC4OsHZCAACAlGCwdkIAAED6srB2QgAAAGAFsXZCAADAxVexdkIAAIArqrF2QgAAQJH8sXZCAAAA906ydkIAAMBcobJ2QgAAgMLzsnZCAABAKEazdkIAAACOmLN2QgAAwPPqs3ZCAACAWT20dkIAAEC/j7R2QgAAACXitHZCAADAijS1dkIAAIDwhrV2QgAAQFbZtXZCAAAAvCu2dkIAAMAhfrZ2QgAAgIfQtnZCAABA7SK3dkIAAABTdbd2QgAAwLjHt3ZCAACAHhq4dkIAAECEbLh2QgAAAOq+uHZCAADATxG5dkIAAIC1Y7l2QgAAQBu2uXZCAAAAgQi6dkIAAMDmWrp2QgAAgEytunZCAABAsv+6dkIAAAAYUrt2QgAAwH2ku3ZCAACA4/a7dkIAAEBJSbx2QgAAAK+bvHZCAADAFO68dkIAAIB6QL12QgAAQOCSvXZCAAAARuW9dkIAAMCrN752QgAAgBGKvnZCAABAd9y+dkIAAADdLr92QgAAwEKBv3ZCAACAqNO/dkIAAEAOJsB2QgAAAHR4wHZCAADA2crAdkIAAIA/HcF2QgAAQKVvwXZCAAAAC8LBdkIAAMBwFMJ2QgAAgNZmwnZCAABAPLnCdkIAAACiC8N2QgAAwAdew3ZCAACAbbDDdkI=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[160]},\"value\":{\"__ndarray__\":\"aHv2RUj/zkVEXadFLzKsRS/10UXuZtBF1dbRRWBLsUXX4+9FYBULRsBE1UWHKMhFC1bRRYiSzUWIbvpFJ3rURZOTdkVrbXxFD7RrRaEdDUYDIARGOcPkRTr5y0UXDYFFM0wvRtqGHUbST9ZF4UbZRYW2qEXPEalFu1F2RYr6zUV/aPtFApgDRqx6rEX9dNdFWR11ReLe1EUcFvNFMTLwRftGFEaFJ9FFT0TiRbP3f0UHHghGFHSvRTAADUZrp8VF4xx6RTz2hkWaIcxFQoACRn/XxUW2gXFFs8P9RZaQgEUnLYhFekTJRUBDqEXnlVNF9zgRRicQTUUUzuJFaCMRRiAYEkaScgZGRUvjRSK6W0XJc1lFhul8RY+YMUZM9wlGNqR7ReQNG0a/aXlFAXFtRZjKgUXXlCZGlHrYRdJqaUUYiQlGbvsyRkn/GkaIHaVFkcMMRlWfgkX82mZFw6PPRe1x50Vj4tVFhuEVRiCv/0XyMBNGftIjRngmgEVqJmVFopEvRmykzUWiDtNFEP2ARYl200XZMexFS5yARQsWKUYx1WhFBdHPRS6Z7UUTXAFGkjLoRSmug0XTiXJFN+wnRtyQoUVleh1GWnSFRYgHekUCthxGK6UfRqMM90X98CVGijfPRW6zwUVaV3lFfo3MRRfOgkW8NQdG/mAlRtXu0EXl9N1FA7n4RQyvwkXdZMhFOYL9RUjM70Xph3NFkRH/RcTk0EUk6IFFqv4kRpBha0UPGBJGBLZeRTy+gEXiR19Ff8AbRuIqy0VpqhlGNY79RdaUA0YvfANGlT2nReJrG0Zp84JFXcnxRaRjJUYIu2RF4yu8RY5ChEV2C9BFyJLRRQ==\",\"dtype\":\"float32\",\"order\":\"little\",\"shape\":[160]}},\"selected\":{\"id\":\"1078\"},\"selection_policy\":{\"id\":\"1094\"}},\"id\":\"1077\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"line_color\":\"#30a2da\",\"line_width\":2,\"x\":{\"field\":\"index\"},\"y\":{\"field\":\"value\"}},\"id\":\"1047\",\"type\":\"Line\"},{\"attributes\":{\"line_color\":\"#fc4f30\",\"line_width\":2,\"x\":{\"field\":\"index\"},\"y\":{\"field\":\"value\"}},\"id\":\"1080\",\"type\":\"Line\"},{\"attributes\":{\"line_color\":\"#30a2da\",\"line_width\":2,\"x\":{\"field\":\"index\"},\"y\":{\"field\":\"value\"}},\"id\":\"1076\",\"type\":\"Line\"},{\"attributes\":{},\"id\":\"1013\",\"type\":\"LinearScale\"},{\"attributes\":{\"margin\":[5,5,5,5],\"name\":\"HSpacer01705\",\"sizing_mode\":\"stretch_width\"},\"id\":\"1003\",\"type\":\"Spacer\"},{\"attributes\":{\"children\":[{\"id\":\"1003\"},{\"id\":\"1008\"},{\"id\":\"1303\"}],\"margin\":[0,0,0,0],\"name\":\"Row01701\",\"tags\":[\"embedded\"]},\"id\":\"1002\",\"type\":\"Row\"},{\"attributes\":{\"callback\":null,\"formatters\":{\"@{index}\":\"datetime\"},\"renderers\":[{\"id\":\"1050\"},{\"id\":\"1083\"}],\"tags\":[\"hv_created\"],\"tooltips\":[[\"Variable\",\"@{Variable}\"],[\"index\",\"@{index}{%F %T}\"],[\"value\",\"@{value}\"]]},\"id\":\"1006\",\"type\":\"HoverTool\"},{\"attributes\":{},\"id\":\"1042\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{},\"id\":\"1040\",\"type\":\"AllLabels\"},{\"attributes\":{},\"id\":\"1015\",\"type\":\"LinearScale\"},{\"attributes\":{\"axis\":{\"id\":\"1017\"},\"coordinates\":null,\"grid_line_color\":null,\"group\":null,\"ticker\":null},\"id\":\"1020\",\"type\":\"Grid\"},{\"attributes\":{\"coordinates\":null,\"data_source\":{\"id\":\"1044\"},\"glyph\":{\"id\":\"1047\"},\"group\":null,\"hover_glyph\":null,\"muted_glyph\":{\"id\":\"1049\"},\"nonselection_glyph\":{\"id\":\"1048\"},\"selection_glyph\":{\"id\":\"1076\"},\"view\":{\"id\":\"1051\"}},\"id\":\"1050\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"line_alpha\":0.2,\"line_color\":\"#fc4f30\",\"line_width\":2,\"x\":{\"field\":\"index\"},\"y\":{\"field\":\"value\"}},\"id\":\"1082\",\"type\":\"Line\"},{\"attributes\":{\"axis_label\":\"Price\",\"coordinates\":null,\"formatter\":{\"id\":\"1042\"},\"group\":null,\"major_label_policy\":{\"id\":\"1043\"},\"ticker\":{\"id\":\"1022\"}},\"id\":\"1021\",\"type\":\"LinearAxis\"},{\"attributes\":{\"data\":{\"Variable\":[\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\"],\"index\":{\"__ndarray__\":\"AABAO4OQdkIAAACh1ZB2QgAAwAYokXZCAACAbHqRdkIAAEDSzJF2QgAAADgfknZCAADAnXGSdkIAAIADxJJ2QgAAQGkWk3ZCAAAAz2iTdkIAAMA0u5N2QgAAgJoNlHZCAABAAGCUdkIAAABmspR2QgAAwMsElXZCAACAMVeVdkIAAECXqZV2QgAAAP37lXZCAADAYk6WdkIAAIDIoJZ2QgAAQC7zlnZCAAAAlEWXdkIAAMD5l5d2QgAAgF/ql3ZCAABAxTyYdkIAAAArj5h2QgAAwJDhmHZCAACA9jOZdkIAAEBchpl2QgAAAMLYmXZCAADAJyuadkIAAICNfZp2QgAAQPPPmnZCAAAAWSKbdkIAAMC+dJt2QgAAgCTHm3ZCAABAihmcdkIAAADwa5x2QgAAwFW+nHZCAACAuxCddkIAAEAhY512QgAAAIe1nXZCAADA7AeedkIAAIBSWp52QgAAQLisnnZCAAAAHv+edkIAAMCDUZ92QgAAgOmjn3ZCAABAT/afdkIAAAC1SKB2QgAAwBqboHZCAACAgO2gdkIAAEDmP6F2QgAAAEySoXZCAADAseShdkIAAIAXN6J2QgAAQH2JonZCAAAA49uidkIAAMBILqN2QgAAgK6Ao3ZCAABAFNOjdkIAAAB6JaR2QgAAwN93pHZCAACARcqkdkIAAECrHKV2QgAAABFvpXZCAADAdsGldkIAAIDcE6Z2QgAAQEJmpnZCAAAAqLimdkIAAMANC6d2QgAAgHNdp3ZCAABA2a+ndkIAAAA/Aqh2QgAAwKRUqHZCAACACqeodkIAAEBw+ah2QgAAANZLqXZCAADAO56pdkIAAICh8Kl2QgAAQAdDqnZCAAAAbZWqdkIAAMDS56p2QgAAgDg6q3ZCAABAnoyrdkIAAAAE36t2QgAAwGkxrHZCAACAz4OsdkIAAEA11qx2QgAAAJsorXZCAADAAHutdkIAAIBmza12QgAAQMwfrnZCAAAAMnKudkIAAMCXxK52QgAAgP0Wr3ZCAABAY2mvdkIAAADJu692QgAAwC4OsHZCAACAlGCwdkIAAED6srB2QgAAAGAFsXZCAADAxVexdkIAAIArqrF2QgAAQJH8sXZCAAAA906ydkIAAMBcobJ2QgAAgMLzsnZCAABAKEazdkIAAACOmLN2QgAAwPPqs3ZCAACAWT20dkIAAEC/j7R2QgAAACXitHZCAADAijS1dkIAAIDwhrV2QgAAQFbZtXZCAAAAvCu2dkIAAMAhfrZ2QgAAgIfQtnZCAABA7SK3dkIAAABTdbd2QgAAwLjHt3ZCAACAHhq4dkIAAECEbLh2QgAAAOq+uHZCAADATxG5dkIAAIC1Y7l2QgAAQBu2uXZCAAAAgQi6dkIAAMDmWrp2QgAAgEytunZCAABAsv+6dkIAAAAYUrt2QgAAwH2ku3ZCAACA4/a7dkIAAEBJSbx2QgAAAK+bvHZCAADAFO68dkIAAIB6QL12QgAAQOCSvXZCAAAARuW9dkIAAMCrN752QgAAgBGKvnZCAABAd9y+dkIAAADdLr92QgAAwEKBv3ZCAACAqNO/dkIAAEAOJsB2QgAAAHR4wHZCAADA2crAdkIAAIA/HcF2QgAAQKVvwXZCAAAAC8LBdkIAAMBwFMJ2QgAAgNZmwnZCAABAPLnCdkIAAACiC8N2QgAAwAdew3ZCAACAbbDDdkI=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[160]},\"value\":{\"__ndarray__\":\"08H6PzNPvUAtPgXADLe5QP////+/qbRAvjEEABdytECy2vy/3ui5QLwi+N86U7lAKy/5n3BmuUCdSgaAQu+1QJ1KBoDCxbpAIef9fxSQwECR8/4/yue5QLwi+N+6hbhAkfP+P8qzuUCFqwMgLgi5QAAAAACAxL5A1dAGYI9dukAf2PFfOFWuQIarAyAuka1AY7X5f70rrEDpYP2fB7LAQNPB+j9zy75Astr8v17nukCbO/pf5mm5QOEnDqBHcK9AeVT83yNpxkB7VPzfERbDQGO1+X+9JblAGJ8CYHijt0C7Ivjfem20QGXEBaCZxrRA4CcOoMdGrUBNJQNAId64QE4lA0Ch8b5AkPP+P4pWwUAXnwJguHy1QGXEBaDZorhAvCL43/rdrUAXnwJguFi5QCHn/X8UYrxALT4FwIxqvUDpYP2f2QzAQNTQBmAPLrlACkj7H1x+vkBE3QcgBQuuQOlg/Z/HSsFAIef9f5ROtUCGqwMgrjvBQHpU/N+R/bdAZcQFoBkMr0AKSPsf3C+wQELO+/+o+rhAeVT832MMwED2twTgY8i3QKmhDcAeqq1AelT831HtvkDKiAtAsymvQB/Y8V84HK5ACkj7H1wXvED2twTgo+a0QETdByAFQalAAAAAAOCYw0BXXvI/YbirQJ1KBoDCVrtAyHn/H0UgwkDIef8fhQjDQFlt/l/vtcBA6WD9n8chu0DfGAKAaxarQN0J9l8PmKxAeUXwv/Wbr0BZbf5frwrHQN8YAoBrjMBAmzv6X2aqrEDfGAKAXebDQJ1KBoDC061AelT839FCrEAAAAAAgJGvQBefAmA4L8RA08H6P7MuukAj9gmgcAmsQIarAyDcc8FAelT836M4yUCy2vy/LMfEQJ1KBoBCdLRAF58CYGZzwUC8IvjfepqvQOEnDqBHyqxAbwwBwLUFuUD2twTgY4K7QIarAyBuprhATiUDQBPbwUBvDAHA9cu8QIarAyAcpMFATiUDQBM/w0DdCfZfjxGuQOEnDqDH/atA6WD9n4chyEAKSPsfHKW4QETdByAFyrlA3xgCgGtOr0BE3Qcgha65QAAAAAAAJr5A3Qn2Xw8fr0AWnwJguGfCQL4xBADXraxA3xgCgCtGuUC8IvjfOsK8QL4xBADJFcBALT4FwAxmvECdSgaAQk+vQLwi+N96qK5AWW3+X+8qxEC8IvjfOrezQMh5/x/F4MRAvjEEAFdTr0Af2PFfuPetQE4lA0AT8MJAOIYA4JqsxUCR8/4/Sj6/QDiGAOD6O8NATSUDQCEeukAuPgXAzDu4QFde8j/hUq5AF58CYHhttkBOJQNAIRawQJHz/j8Kh79ATiUDQCExxUCaO/pf5n24QHpU/N8R17pACkj7H1x/vkAAAAAAgA+2QN4YAoDrsrhAnDv6X+a6vUDV0AZgD7u8QHhF8L/17q1AhqsDIO4Lv0AKSPsfXPq4QAIPDCDcOa9AWW3+X69YxUDdCfZfjyOtQLLa/L9soMJAY7X5fz3zqkC+MQQAFzWwQETdByCFiKxA6WD9n7k7w0AtPgXATO24QE4lA0DzOsNAsdr8v57Cu0ABAAAAQAG/QCDn/X/0G8BA/////3/Zs0BvDAHAddbCQIarAyCuoK9AvjEEAOllwECy2vy//jPFQLwi+N96SqxA3xgCgCt0tkArL/mfsBCwQELO+//oqbhAIef9f5RtuUA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[160]}},\"selected\":{\"id\":\"1045\"},\"selection_policy\":{\"id\":\"1059\"}},\"id\":\"1044\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"line_alpha\":0.1,\"line_color\":\"#30a2da\",\"line_width\":2,\"x\":{\"field\":\"index\"},\"y\":{\"field\":\"value\"}},\"id\":\"1048\",\"type\":\"Line\"},{\"attributes\":{\"line_alpha\":0.1,\"line_color\":\"#fc4f30\",\"line_width\":2,\"x\":{\"field\":\"index\"},\"y\":{\"field\":\"value\"}},\"id\":\"1081\",\"type\":\"Line\"},{\"attributes\":{\"line_color\":\"#fc4f30\",\"line_width\":2,\"x\":{\"field\":\"index\"},\"y\":{\"field\":\"value\"}},\"id\":\"1110\",\"type\":\"Line\"},{\"attributes\":{\"source\":{\"id\":\"1077\"}},\"id\":\"1084\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"1022\",\"type\":\"BasicTicker\"},{\"attributes\":{\"coordinates\":null,\"group\":null,\"text_color\":\"black\",\"text_font_size\":\"12pt\"},\"id\":\"1009\",\"type\":\"Title\"},{\"attributes\":{},\"id\":\"1078\",\"type\":\"Selection\"},{\"attributes\":{\"mantissas\":[1,2,5],\"max_interval\":500.0,\"num_minor_ticks\":0},\"id\":\"1062\",\"type\":\"AdaptiveTicker\"},{\"attributes\":{\"num_minor_ticks\":5,\"tickers\":[{\"id\":\"1062\"},{\"id\":\"1063\"},{\"id\":\"1064\"},{\"id\":\"1065\"},{\"id\":\"1066\"},{\"id\":\"1067\"},{\"id\":\"1068\"},{\"id\":\"1069\"},{\"id\":\"1070\"},{\"id\":\"1071\"},{\"id\":\"1072\"},{\"id\":\"1073\"}]},\"id\":\"1018\",\"type\":\"DatetimeTicker\"},{\"attributes\":{},\"id\":\"1043\",\"type\":\"AllLabels\"},{\"attributes\":{\"base\":60,\"mantissas\":[1,2,5,10,15,20,30],\"max_interval\":1800000.0,\"min_interval\":1000.0,\"num_minor_ticks\":0},\"id\":\"1063\",\"type\":\"AdaptiveTicker\"},{\"attributes\":{\"axis\":{\"id\":\"1021\"},\"coordinates\":null,\"dimension\":1,\"grid_line_color\":null,\"group\":null,\"ticker\":null},\"id\":\"1024\",\"type\":\"Grid\"},{\"attributes\":{\"coordinates\":null,\"data_source\":{\"id\":\"1077\"},\"glyph\":{\"id\":\"1080\"},\"group\":null,\"hover_glyph\":null,\"muted_glyph\":{\"id\":\"1082\"},\"nonselection_glyph\":{\"id\":\"1081\"},\"selection_glyph\":{\"id\":\"1110\"},\"view\":{\"id\":\"1084\"}},\"id\":\"1083\",\"type\":\"GlyphRenderer\"},{\"attributes\":{},\"id\":\"1073\",\"type\":\"YearsTicker\"},{\"attributes\":{\"label\":{\"value\":\"Real\"},\"renderers\":[{\"id\":\"1050\"}]},\"id\":\"1075\",\"type\":\"LegendItem\"},{\"attributes\":{\"base\":24,\"mantissas\":[1,2,4,6,8,12],\"max_interval\":43200000.0,\"min_interval\":3600000.0,\"num_minor_ticks\":0},\"id\":\"1064\",\"type\":\"AdaptiveTicker\"},{\"attributes\":{\"months\":[0,1,2,3,4,5,6,7,8,9,10,11]},\"id\":\"1069\",\"type\":\"MonthsTicker\"},{\"attributes\":{\"days\":[1,15]},\"id\":\"1068\",\"type\":\"DaysTicker\"},{\"attributes\":{},\"id\":\"1027\",\"type\":\"WheelZoomTool\"},{\"attributes\":{\"label\":{\"value\":\"Predicted\"},\"renderers\":[{\"id\":\"1083\"}]},\"id\":\"1109\",\"type\":\"LegendItem\"},{\"attributes\":{\"days\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31]},\"id\":\"1065\",\"type\":\"DaysTicker\"},{\"attributes\":{},\"id\":\"1025\",\"type\":\"SaveTool\"},{\"attributes\":{\"days\":[1,4,7,10,13,16,19,22,25,28]},\"id\":\"1066\",\"type\":\"DaysTicker\"},{\"attributes\":{\"days\":[1,8,15,22]},\"id\":\"1067\",\"type\":\"DaysTicker\"},{\"attributes\":{\"overlay\":{\"id\":\"1030\"}},\"id\":\"1028\",\"type\":\"BoxZoomTool\"},{\"attributes\":{\"source\":{\"id\":\"1044\"}},\"id\":\"1051\",\"type\":\"CDSView\"},{\"attributes\":{\"months\":[0,6]},\"id\":\"1072\",\"type\":\"MonthsTicker\"},{\"attributes\":{},\"id\":\"1029\",\"type\":\"ResetTool\"},{\"attributes\":{},\"id\":\"1094\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"months\":[0,2,4,6,8,10]},\"id\":\"1070\",\"type\":\"MonthsTicker\"},{\"attributes\":{\"bottom_units\":\"screen\",\"coordinates\":null,\"fill_alpha\":0.5,\"fill_color\":\"lightgrey\",\"group\":null,\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":1.0,\"line_color\":\"black\",\"line_dash\":[4,4],\"line_width\":2,\"right_units\":\"screen\",\"syncable\":false,\"top_units\":\"screen\"},\"id\":\"1030\",\"type\":\"BoxAnnotation\"},{\"attributes\":{\"line_alpha\":0.2,\"line_color\":\"#30a2da\",\"line_width\":2,\"x\":{\"field\":\"index\"},\"y\":{\"field\":\"value\"}},\"id\":\"1049\",\"type\":\"Line\"},{\"attributes\":{\"months\":[0,4,8]},\"id\":\"1071\",\"type\":\"MonthsTicker\"},{\"attributes\":{},\"id\":\"1059\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"below\":[{\"id\":\"1017\"}],\"center\":[{\"id\":\"1020\"},{\"id\":\"1024\"}],\"height\":300,\"left\":[{\"id\":\"1021\"}],\"margin\":[5,5,5,5],\"min_border_bottom\":10,\"min_border_left\":10,\"min_border_right\":10,\"min_border_top\":10,\"renderers\":[{\"id\":\"1050\"},{\"id\":\"1083\"}],\"right\":[{\"id\":\"1074\"}],\"sizing_mode\":\"fixed\",\"title\":{\"id\":\"1009\"},\"toolbar\":{\"id\":\"1031\"},\"width\":700,\"x_range\":{\"id\":\"1004\"},\"x_scale\":{\"id\":\"1013\"},\"y_range\":{\"id\":\"1005\"},\"y_scale\":{\"id\":\"1015\"}},\"id\":\"1008\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{\"end\":1564358400000.0,\"reset_end\":1564358400000.0,\"reset_start\":1550620800000.0,\"start\":1550620800000.0,\"tags\":[[[\"index\",\"index\",null]]]},\"id\":\"1004\",\"type\":\"Range1d\"},{\"attributes\":{},\"id\":\"1039\",\"type\":\"DatetimeTickFormatter\"},{\"attributes\":{\"tools\":[{\"id\":\"1006\"},{\"id\":\"1025\"},{\"id\":\"1026\"},{\"id\":\"1027\"},{\"id\":\"1028\"},{\"id\":\"1029\"}]},\"id\":\"1031\",\"type\":\"Toolbar\"},{\"attributes\":{\"end\":13881.3572993,\"reset_end\":13881.3572993,\"reset_start\":2264.4329837,\"start\":2264.4329837,\"tags\":[[[\"value\",\"value\",null]]]},\"id\":\"1005\",\"type\":\"Range1d\"}],\"root_ids\":[\"1002\"]},\"title\":\"Bokeh Application\",\"version\":\"2.4.2\"}};\n",
       "    var render_items = [{\"docid\":\"92afe17d-448b-4b33-938b-3d474052f139\",\"root_ids\":[\"1002\"],\"roots\":{\"1002\":\"b50d63d5-39fa-4712-89bf-f76fec7cb20a\"}}];\n",
       "    root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "  }\n",
       "  if (root.Bokeh !== undefined && root.Bokeh.Panel !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined && root.Bokeh.Panel !== undefined) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else if (document.readyState == \"complete\") {\n",
       "        attempts++;\n",
       "        if (attempts > 200) {\n",
       "          clearInterval(timer);\n",
       "          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "        }\n",
       "      }\n",
       "    }, 25, root)\n",
       "  }\n",
       "})(window);</script>"
      ],
      "text/plain": [
       ":NdOverlay   [Variable]\n",
       "   :Curve   [index]   (value)"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "application/vnd.holoviews_exec.v0+json": {
       "id": "1002"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plot the real vs predicted values as a line chart\n",
    "stocks.hvplot.line(xlabel=\"Date\", ylabel=\"Price\")"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "vscode": {
   "interpreter": {
    "hash": "f7002163cacfad24393b7620eed21e926662f31a9c25cb71446747eea29a57e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
