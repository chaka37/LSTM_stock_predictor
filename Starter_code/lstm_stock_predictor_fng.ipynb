{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Stock Predictor Using Fear and Greed Index\n",
    "\n",
    "In this notebook, you will build and train a custom LSTM RNN that uses a 10 day window of Bitcoin fear and greed index values to predict the 11th day closing price. \n",
    "\n",
    "You will need to:\n",
    "\n",
    "1. Prepare the data for training and testing\n",
    "2. Build and train a custom LSTM RNN\n",
    "3. Evaluate the performance of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "In this section, you will need to prepare the training and testing data for the model. The model will use a rolling 10 day window to predict the 11th day closing price.\n",
    "\n",
    "You will need to:\n",
    "1. Use the `window_data` function to generate the X and y values for the model.\n",
    "2. Split the data into 70% training and 30% testing\n",
    "3. Apply the MinMaxScaler to the X and y values\n",
    "4. Reshape the X_train and X_test data for the model. Note: The required input format for the LSTM is:\n",
    "\n",
    "```python\n",
    "reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hvplot.pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed for reproducibility\n",
    "# Note: This is for the homework solution, but it is good practice to comment this out and run multiple experiments to evaluate your model\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import random\n",
    "random.set_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fng_value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-07-29</th>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-28</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-27</th>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-26</th>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-25</th>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             fng_value\n",
       "date                  \n",
       "2019-07-29          19\n",
       "2019-07-28          16\n",
       "2019-07-27          47\n",
       "2019-07-26          24\n",
       "2019-07-25          42"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the fear and greed sentiment data for Bitcoin\n",
    "df = pd.read_csv('btc_sentiment.csv', index_col=\"date\", infer_datetime_format=True, parse_dates=True)\n",
    "df = df.drop(columns=\"fng_classification\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date\n",
       "2019-07-25    9882.429688\n",
       "2019-07-26    9847.450195\n",
       "2019-07-27    9478.320313\n",
       "2019-07-28    9531.769531\n",
       "2019-07-29    9529.889648\n",
       "Name: Close, dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the historical closing prices for Bitcoin\n",
    "df2 = pd.read_csv('btc_historic.csv', index_col=\"Date\", infer_datetime_format=True, parse_dates=True)['Close']\n",
    "df2 = df2.sort_index()\n",
    "df2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fng_value</th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-07-25</th>\n",
       "      <td>42</td>\n",
       "      <td>9882.429688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-26</th>\n",
       "      <td>24</td>\n",
       "      <td>9847.450195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-27</th>\n",
       "      <td>47</td>\n",
       "      <td>9478.320313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-28</th>\n",
       "      <td>16</td>\n",
       "      <td>9531.769531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-29</th>\n",
       "      <td>19</td>\n",
       "      <td>9529.889648</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             fng_value        Close\n",
       "2019-07-25          42  9882.429688\n",
       "2019-07-26          24  9847.450195\n",
       "2019-07-27          47  9478.320313\n",
       "2019-07-28          16  9531.769531\n",
       "2019-07-29          19  9529.889648"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Join the data into a single DataFrame\n",
    "df = df.join(df2, how=\"inner\")\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fng_value</th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-02-01</th>\n",
       "      <td>30</td>\n",
       "      <td>9114.719727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-02</th>\n",
       "      <td>15</td>\n",
       "      <td>8870.820313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-03</th>\n",
       "      <td>40</td>\n",
       "      <td>9251.269531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-04</th>\n",
       "      <td>24</td>\n",
       "      <td>8218.049805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-05</th>\n",
       "      <td>11</td>\n",
       "      <td>6937.080078</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             fng_value        Close\n",
       "2018-02-01          30  9114.719727\n",
       "2018-02-02          15  8870.820313\n",
       "2018-02-03          40  9251.269531\n",
       "2018-02-04          24  8218.049805\n",
       "2018-02-05          11  6937.080078"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function accepts the column number for the features (X) and the target (y)\n",
    "# It chunks the data up with a rolling window of Xt-n to predict Xt\n",
    "# It returns a numpy array of X any y\n",
    "def window_data(df, window, feature_col_number, target_col_number):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(len(df) - window - 1):\n",
    "        features = df.iloc[i:(i + window), feature_col_number]\n",
    "        target = df.iloc[(i + window), target_col_number]\n",
    "        X.append(features)\n",
    "        y.append(target)\n",
    "    return np.array(X), np.array(y).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict Closing Prices using a 10 day window of previous fng values\n",
    "# Then, experiment with window sizes anywhere from 1 to 10 and see how the model performance changes\n",
    "window_size = 10\n",
    "\n",
    "# Column index 0 is the 'fng_value' column\n",
    "# Column index 1 is the `Close` column\n",
    "feature_column = 0\n",
    "target_column = 1\n",
    "X, y = window_data(df, window_size, feature_column, target_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 70% of the data for training and the remaineder for testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Use the MinMaxScaler to scale data between 0 and 1.\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaler.fit(X)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "scaler.fit(y)\n",
    "y_train = scaler.transform(y_train)\n",
    "y_test = scaler.transform(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the features for the model\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Train the LSTM RNN\n",
    "\n",
    "In this section, you will design a custom LSTM RNN and fit (train) it using the training data.\n",
    "\n",
    "You will need to:\n",
    "1. Define the model architecture\n",
    "2. Compile the model\n",
    "3. Fit the model to the training data\n",
    "\n",
    "### Hints:\n",
    "You will want to use the same model architecture and random seed for both notebooks. This is necessary to accurately compare the performance of the FNG model vs the closing price model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the LSTM model. \n",
    "# The return sequences need to be set to True if you are adding additional LSTM layers, but \n",
    "# You don't have to do this for the final layer. \n",
    "# Note: The dropouts help prevent overfitting\n",
    "# Note: The input shape is the number of time steps and the number of indicators\n",
    "# Note: Batching inputs has a different input shape of Samples/TimeSteps/Features\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "dropout_fraction = 0.2\n",
    "\n",
    "# Layer 1\n",
    "model.add(LSTM(\n",
    "    units = 100,\n",
    "    return_sequences = True,\n",
    "    input_shape = (X_train.shape[1],1))\n",
    "         )\n",
    "model.add(Dropout(dropout_fraction))\n",
    "\n",
    "# Layer 2\n",
    "model.add(LSTM(\n",
    "    units = 50,\n",
    "    return_sequences = True,\n",
    "        ))\n",
    "model.add(Dropout(dropout_fraction))\n",
    "\n",
    "# Layer 3\n",
    "model.add(LSTM(\n",
    "    units = 40,\n",
    "    return_sequences = False,\n",
    "        ))\n",
    "model.add(Dropout(dropout_fraction))\n",
    "\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_3 (LSTM)               (None, 10, 100)           40800     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 10, 100)           0         \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 10, 50)            30200     \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 10, 50)            0         \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 40)                14560     \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 40)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 41        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 85,601\n",
      "Trainable params: 85,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Summarize the model\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "12/12 [==============================] - 8s 17ms/step - loss: 0.0856\n",
      "Epoch 2/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0476\n",
      "Epoch 3/250\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.0467\n",
      "Epoch 4/250\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.0457\n",
      "Epoch 5/250\n",
      "12/12 [==============================] - 0s 21ms/step - loss: 0.0456\n",
      "Epoch 6/250\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.0462\n",
      "Epoch 7/250\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0453\n",
      "Epoch 8/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0443\n",
      "Epoch 9/250\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.0446\n",
      "Epoch 10/250\n",
      "12/12 [==============================] - 0s 25ms/step - loss: 0.0468\n",
      "Epoch 11/250\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0453\n",
      "Epoch 12/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0462\n",
      "Epoch 13/250\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.0450\n",
      "Epoch 14/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0447\n",
      "Epoch 15/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0445\n",
      "Epoch 16/250\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0445\n",
      "Epoch 17/250\n",
      "12/12 [==============================] - 0s 22ms/step - loss: 0.0451\n",
      "Epoch 18/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0453\n",
      "Epoch 19/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0458\n",
      "Epoch 20/250\n",
      "12/12 [==============================] - 0s 29ms/step - loss: 0.0462\n",
      "Epoch 21/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0453\n",
      "Epoch 22/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0447\n",
      "Epoch 23/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0449\n",
      "Epoch 24/250\n",
      "12/12 [==============================] - 0s 21ms/step - loss: 0.0432\n",
      "Epoch 25/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0449\n",
      "Epoch 26/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0448\n",
      "Epoch 27/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0451\n",
      "Epoch 28/250\n",
      "12/12 [==============================] - 0s 21ms/step - loss: 0.0434\n",
      "Epoch 29/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0438\n",
      "Epoch 30/250\n",
      "12/12 [==============================] - 0s 22ms/step - loss: 0.0437\n",
      "Epoch 31/250\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0426\n",
      "Epoch 32/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0436\n",
      "Epoch 33/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0445\n",
      "Epoch 34/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0429\n",
      "Epoch 35/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0442\n",
      "Epoch 36/250\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.0432\n",
      "Epoch 37/250\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0431\n",
      "Epoch 38/250\n",
      "12/12 [==============================] - 0s 20ms/step - loss: 0.0431\n",
      "Epoch 39/250\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0420\n",
      "Epoch 40/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0421\n",
      "Epoch 41/250\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0424\n",
      "Epoch 42/250\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.0427\n",
      "Epoch 43/250\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0416\n",
      "Epoch 44/250\n",
      "12/12 [==============================] - 0s 21ms/step - loss: 0.0417\n",
      "Epoch 45/250\n",
      "12/12 [==============================] - 0s 21ms/step - loss: 0.0430\n",
      "Epoch 46/250\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0418\n",
      "Epoch 47/250\n",
      "12/12 [==============================] - 0s 21ms/step - loss: 0.0437\n",
      "Epoch 48/250\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0428\n",
      "Epoch 49/250\n",
      "12/12 [==============================] - 0s 25ms/step - loss: 0.0410\n",
      "Epoch 50/250\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.0421\n",
      "Epoch 51/250\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0425\n",
      "Epoch 52/250\n",
      "12/12 [==============================] - 0s 20ms/step - loss: 0.0427\n",
      "Epoch 53/250\n",
      "12/12 [==============================] - 0s 41ms/step - loss: 0.0428\n",
      "Epoch 54/250\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.0427\n",
      "Epoch 55/250\n",
      "12/12 [==============================] - 0s 21ms/step - loss: 0.0431\n",
      "Epoch 56/250\n",
      "12/12 [==============================] - 0s 20ms/step - loss: 0.0423\n",
      "Epoch 57/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0422\n",
      "Epoch 58/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0425\n",
      "Epoch 59/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0424\n",
      "Epoch 60/250\n",
      "12/12 [==============================] - 0s 25ms/step - loss: 0.0438\n",
      "Epoch 61/250\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0409\n",
      "Epoch 62/250\n",
      "12/12 [==============================] - 0s 22ms/step - loss: 0.0418\n",
      "Epoch 63/250\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.0416\n",
      "Epoch 64/250\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.0415\n",
      "Epoch 65/250\n",
      "12/12 [==============================] - 0s 21ms/step - loss: 0.0425\n",
      "Epoch 66/250\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0428\n",
      "Epoch 67/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0422\n",
      "Epoch 68/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0424\n",
      "Epoch 69/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0420\n",
      "Epoch 70/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0425\n",
      "Epoch 71/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0411\n",
      "Epoch 72/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0417\n",
      "Epoch 73/250\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0402\n",
      "Epoch 74/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0406\n",
      "Epoch 75/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0403\n",
      "Epoch 76/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0398\n",
      "Epoch 77/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0415\n",
      "Epoch 78/250\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0406\n",
      "Epoch 79/250\n",
      "12/12 [==============================] - 0s 20ms/step - loss: 0.0397\n",
      "Epoch 80/250\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0403\n",
      "Epoch 81/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0389\n",
      "Epoch 82/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0404\n",
      "Epoch 83/250\n",
      "12/12 [==============================] - 0s 22ms/step - loss: 0.0387\n",
      "Epoch 84/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0417\n",
      "Epoch 85/250\n",
      "12/12 [==============================] - 0s 31ms/step - loss: 0.0407\n",
      "Epoch 86/250\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.0392\n",
      "Epoch 87/250\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.0384\n",
      "Epoch 88/250\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.0390\n",
      "Epoch 89/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0398\n",
      "Epoch 90/250\n",
      "12/12 [==============================] - 0s 21ms/step - loss: 0.0405\n",
      "Epoch 91/250\n",
      "12/12 [==============================] - 0s 21ms/step - loss: 0.0383\n",
      "Epoch 92/250\n",
      "12/12 [==============================] - 0s 22ms/step - loss: 0.0388\n",
      "Epoch 93/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0392\n",
      "Epoch 94/250\n",
      "12/12 [==============================] - 0s 21ms/step - loss: 0.0383\n",
      "Epoch 95/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0383\n",
      "Epoch 96/250\n",
      "12/12 [==============================] - 0s 22ms/step - loss: 0.0382\n",
      "Epoch 97/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0388\n",
      "Epoch 98/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0389\n",
      "Epoch 99/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0404\n",
      "Epoch 100/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0388\n",
      "Epoch 101/250\n",
      "12/12 [==============================] - 0s 21ms/step - loss: 0.0383\n",
      "Epoch 102/250\n",
      "12/12 [==============================] - 0s 21ms/step - loss: 0.0383\n",
      "Epoch 103/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0391\n",
      "Epoch 104/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0391\n",
      "Epoch 105/250\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.0393\n",
      "Epoch 106/250\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0394\n",
      "Epoch 107/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0380\n",
      "Epoch 108/250\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0387\n",
      "Epoch 109/250\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.0398\n",
      "Epoch 110/250\n",
      "12/12 [==============================] - 0s 22ms/step - loss: 0.0374\n",
      "Epoch 111/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0384\n",
      "Epoch 112/250\n",
      "12/12 [==============================] - 0s 22ms/step - loss: 0.0388\n",
      "Epoch 113/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0380\n",
      "Epoch 114/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0406\n",
      "Epoch 115/250\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.0389\n",
      "Epoch 116/250\n",
      "12/12 [==============================] - 0s 28ms/step - loss: 0.0375\n",
      "Epoch 117/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0388\n",
      "Epoch 118/250\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.0388\n",
      "Epoch 119/250\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.0386\n",
      "Epoch 120/250\n",
      "12/12 [==============================] - 0s 22ms/step - loss: 0.0391\n",
      "Epoch 121/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0396\n",
      "Epoch 122/250\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.0375\n",
      "Epoch 123/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0385\n",
      "Epoch 124/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0389\n",
      "Epoch 125/250\n",
      "12/12 [==============================] - 0s 28ms/step - loss: 0.0379\n",
      "Epoch 126/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0386\n",
      "Epoch 127/250\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.0382\n",
      "Epoch 128/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0379\n",
      "Epoch 129/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0396\n",
      "Epoch 130/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0380\n",
      "Epoch 131/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0388\n",
      "Epoch 132/250\n",
      "12/12 [==============================] - 0s 21ms/step - loss: 0.0383\n",
      "Epoch 133/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0393\n",
      "Epoch 134/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0382\n",
      "Epoch 135/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0388\n",
      "Epoch 136/250\n",
      "12/12 [==============================] - 0s 21ms/step - loss: 0.0389\n",
      "Epoch 137/250\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0378\n",
      "Epoch 138/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0396\n",
      "Epoch 139/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0376\n",
      "Epoch 140/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0389\n",
      "Epoch 141/250\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0380\n",
      "Epoch 142/250\n",
      "12/12 [==============================] - 0s 21ms/step - loss: 0.0380\n",
      "Epoch 143/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0377\n",
      "Epoch 144/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0381\n",
      "Epoch 145/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0397\n",
      "Epoch 146/250\n",
      "12/12 [==============================] - 0s 22ms/step - loss: 0.0383\n",
      "Epoch 147/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0377\n",
      "Epoch 148/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0365\n",
      "Epoch 149/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0376\n",
      "Epoch 150/250\n",
      "12/12 [==============================] - 0s 22ms/step - loss: 0.0377\n",
      "Epoch 151/250\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0375\n",
      "Epoch 152/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0380\n",
      "Epoch 153/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0382\n",
      "Epoch 154/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0379\n",
      "Epoch 155/250\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0381\n",
      "Epoch 156/250\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 0.0375\n",
      "Epoch 157/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0377\n",
      "Epoch 158/250\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0379\n",
      "Epoch 159/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0382\n",
      "Epoch 160/250\n",
      "12/12 [==============================] - 0s 22ms/step - loss: 0.0387\n",
      "Epoch 161/250\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.0388\n",
      "Epoch 162/250\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0374\n",
      "Epoch 163/250\n",
      "12/12 [==============================] - 0s 28ms/step - loss: 0.0384\n",
      "Epoch 164/250\n",
      "12/12 [==============================] - 0s 20ms/step - loss: 0.0379\n",
      "Epoch 165/250\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0378\n",
      "Epoch 166/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0378\n",
      "Epoch 167/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0379\n",
      "Epoch 168/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0370\n",
      "Epoch 169/250\n",
      "12/12 [==============================] - 0s 22ms/step - loss: 0.0370\n",
      "Epoch 170/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0369\n",
      "Epoch 171/250\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.0367\n",
      "Epoch 172/250\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0376\n",
      "Epoch 173/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0386\n",
      "Epoch 174/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0370\n",
      "Epoch 175/250\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.0374\n",
      "Epoch 176/250\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.0359\n",
      "Epoch 177/250\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.0366\n",
      "Epoch 178/250\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0369\n",
      "Epoch 179/250\n",
      "12/12 [==============================] - 0s 20ms/step - loss: 0.0368\n",
      "Epoch 180/250\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.0363\n",
      "Epoch 181/250\n",
      "12/12 [==============================] - 0s 20ms/step - loss: 0.0368\n",
      "Epoch 182/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0374\n",
      "Epoch 183/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0368\n",
      "Epoch 184/250\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0381\n",
      "Epoch 185/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0377\n",
      "Epoch 186/250\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0367\n",
      "Epoch 187/250\n",
      "12/12 [==============================] - 0s 20ms/step - loss: 0.0374\n",
      "Epoch 188/250\n",
      "12/12 [==============================] - 0s 22ms/step - loss: 0.0368\n",
      "Epoch 189/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0373\n",
      "Epoch 190/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0374\n",
      "Epoch 191/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0363\n",
      "Epoch 192/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0371\n",
      "Epoch 193/250\n",
      "12/12 [==============================] - 0s 21ms/step - loss: 0.0369\n",
      "Epoch 194/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0379\n",
      "Epoch 195/250\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.0372\n",
      "Epoch 196/250\n",
      "12/12 [==============================] - 0s 20ms/step - loss: 0.0371\n",
      "Epoch 197/250\n",
      "12/12 [==============================] - 0s 31ms/step - loss: 0.0376\n",
      "Epoch 198/250\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.0371\n",
      "Epoch 199/250\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.0366\n",
      "Epoch 200/250\n",
      "12/12 [==============================] - 0s 20ms/step - loss: 0.0371\n",
      "Epoch 201/250\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0376\n",
      "Epoch 202/250\n",
      "12/12 [==============================] - 0s 28ms/step - loss: 0.0373\n",
      "Epoch 203/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0393\n",
      "Epoch 204/250\n",
      "12/12 [==============================] - 0s 20ms/step - loss: 0.0375\n",
      "Epoch 205/250\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0355\n",
      "Epoch 206/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0374\n",
      "Epoch 207/250\n",
      "12/12 [==============================] - 0s 20ms/step - loss: 0.0363\n",
      "Epoch 208/250\n",
      "12/12 [==============================] - 0s 20ms/step - loss: 0.0366\n",
      "Epoch 209/250\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.0370\n",
      "Epoch 210/250\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0364\n",
      "Epoch 211/250\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.0374\n",
      "Epoch 212/250\n",
      "12/12 [==============================] - 0s 20ms/step - loss: 0.0366\n",
      "Epoch 213/250\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0366\n",
      "Epoch 214/250\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.0372\n",
      "Epoch 215/250\n",
      "12/12 [==============================] - 0s 22ms/step - loss: 0.0389\n",
      "Epoch 216/250\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.0379\n",
      "Epoch 217/250\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0377\n",
      "Epoch 218/250\n",
      "12/12 [==============================] - 0s 20ms/step - loss: 0.0379\n",
      "Epoch 219/250\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.0358\n",
      "Epoch 220/250\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.0363\n",
      "Epoch 221/250\n",
      "12/12 [==============================] - 0s 22ms/step - loss: 0.0365\n",
      "Epoch 222/250\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.0358\n",
      "Epoch 223/250\n",
      "12/12 [==============================] - 0s 22ms/step - loss: 0.0357\n",
      "Epoch 224/250\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.0364\n",
      "Epoch 225/250\n",
      "12/12 [==============================] - 0s 20ms/step - loss: 0.0366\n",
      "Epoch 226/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0361\n",
      "Epoch 227/250\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.0368\n",
      "Epoch 228/250\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0367\n",
      "Epoch 229/250\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.0368\n",
      "Epoch 230/250\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0374\n",
      "Epoch 231/250\n",
      "12/12 [==============================] - 0s 22ms/step - loss: 0.0368\n",
      "Epoch 232/250\n",
      "12/12 [==============================] - 0s 20ms/step - loss: 0.0359\n",
      "Epoch 233/250\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0373\n",
      "Epoch 234/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0367\n",
      "Epoch 235/250\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.0361\n",
      "Epoch 236/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0373\n",
      "Epoch 237/250\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 0.0366\n",
      "Epoch 238/250\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0361\n",
      "Epoch 239/250\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0363\n",
      "Epoch 240/250\n",
      "12/12 [==============================] - 0s 29ms/step - loss: 0.0363\n",
      "Epoch 241/250\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0354\n",
      "Epoch 242/250\n",
      "12/12 [==============================] - 0s 20ms/step - loss: 0.0361\n",
      "Epoch 243/250\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0363\n",
      "Epoch 244/250\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 0.0371\n",
      "Epoch 245/250\n",
      "12/12 [==============================] - 0s 24ms/step - loss: 0.0364\n",
      "Epoch 246/250\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 0.0354\n",
      "Epoch 247/250\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.0363\n",
      "Epoch 248/250\n",
      "12/12 [==============================] - 0s 27ms/step - loss: 0.0366\n",
      "Epoch 249/250\n",
      "12/12 [==============================] - 0s 19ms/step - loss: 0.0373\n",
      "Epoch 250/250\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 0.0370\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11c06cb6400>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "# Use at least 10 epochs\n",
    "# Do not shuffle the data\n",
    "# Experiement with the batch size, but a smaller batch size is recommended\n",
    "model.fit(X_train, y_train, epochs=250, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance\n",
    "\n",
    "In this section, you will evaluate the model using the test data. \n",
    "\n",
    "You will need to:\n",
    "1. Evaluate the model using the `X_test` and `y_test` data.\n",
    "2. Use the X_test data to make predictions\n",
    "3. Create a DataFrame of Real (y_test) vs predicted values. \n",
    "4. Plot the Real vs predicted values as a line chart\n",
    "\n",
    "### Hints\n",
    "Remember to apply the `inverse_transform` function to the predicted and y_test values to recover the actual closing prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 3s 6ms/step - loss: 0.0420\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.042031604796648026"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make some predictions\n",
    "predicted = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recover the original prices instead of the scaled version\n",
    "predicted_prices = scaler.inverse_transform(predicted)\n",
    "real_prices = scaler.inverse_transform(y_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Real</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-02-20</th>\n",
       "      <td>7503.200195</td>\n",
       "      <td>6457.247070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-21</th>\n",
       "      <td>6583.049805</td>\n",
       "      <td>6091.240234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-22</th>\n",
       "      <td>5289.750000</td>\n",
       "      <td>6822.076172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-23</th>\n",
       "      <td>5234.089844</td>\n",
       "      <td>6183.100098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-24</th>\n",
       "      <td>6632.870117</td>\n",
       "      <td>6232.857422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Real    Predicted\n",
       "2019-02-20  7503.200195  6457.247070\n",
       "2019-02-21  6583.049805  6091.240234\n",
       "2019-02-22  5289.750000  6822.076172\n",
       "2019-02-23  5234.089844  6183.100098\n",
       "2019-02-24  6632.870117  6232.857422"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DataFrame of Real and Predicted values\n",
    "stocks = pd.DataFrame({\n",
    "    \"Real\": real_prices.ravel(),\n",
    "    \"Predicted\": predicted_prices.ravel()\n",
    "}, index = df.index[-len(real_prices): ]) \n",
    "stocks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.holoviews_exec.v0+json": "",
      "text/html": [
       "<div id='1848'>\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "  <div class=\"bk-root\" id=\"b947f30b-82fc-4719-bf2b-c2a430de7c19\" data-root-id=\"1848\"></div>\n",
       "</div>\n",
       "<script type=\"application/javascript\">(function(root) {\n",
       "  function embed_document(root) {\n",
       "    var docs_json = {\"7766cbb3-89d3-4550-a815-f7f483bbe3c8\":{\"defs\":[{\"extends\":null,\"module\":null,\"name\":\"ReactiveHTML1\",\"overrides\":[],\"properties\":[]},{\"extends\":null,\"module\":null,\"name\":\"FlexBox1\",\"overrides\":[],\"properties\":[{\"default\":\"flex-start\",\"kind\":null,\"name\":\"align_content\"},{\"default\":\"flex-start\",\"kind\":null,\"name\":\"align_items\"},{\"default\":\"row\",\"kind\":null,\"name\":\"flex_direction\"},{\"default\":\"wrap\",\"kind\":null,\"name\":\"flex_wrap\"},{\"default\":\"flex-start\",\"kind\":null,\"name\":\"justify_content\"}]},{\"extends\":null,\"module\":null,\"name\":\"GridStack1\",\"overrides\":[],\"properties\":[{\"default\":\"warn\",\"kind\":null,\"name\":\"mode\"},{\"default\":null,\"kind\":null,\"name\":\"ncols\"},{\"default\":null,\"kind\":null,\"name\":\"nrows\"},{\"default\":true,\"kind\":null,\"name\":\"allow_resize\"},{\"default\":true,\"kind\":null,\"name\":\"allow_drag\"},{\"default\":[],\"kind\":null,\"name\":\"state\"}]},{\"extends\":null,\"module\":null,\"name\":\"click1\",\"overrides\":[],\"properties\":[{\"default\":\"\",\"kind\":null,\"name\":\"terminal_output\"},{\"default\":\"\",\"kind\":null,\"name\":\"debug_name\"},{\"default\":0,\"kind\":null,\"name\":\"clears\"}]},{\"extends\":null,\"module\":null,\"name\":\"NotificationAreaBase1\",\"overrides\":[],\"properties\":[{\"default\":\"bottom-right\",\"kind\":null,\"name\":\"position\"},{\"default\":0,\"kind\":null,\"name\":\"_clear\"}]},{\"extends\":null,\"module\":null,\"name\":\"NotificationArea1\",\"overrides\":[],\"properties\":[{\"default\":[],\"kind\":null,\"name\":\"notifications\"},{\"default\":\"bottom-right\",\"kind\":null,\"name\":\"position\"},{\"default\":0,\"kind\":null,\"name\":\"_clear\"},{\"default\":[{\"background\":\"#ffc107\",\"icon\":{\"className\":\"fas fa-exclamation-triangle\",\"color\":\"white\",\"tagName\":\"i\"},\"type\":\"warning\"},{\"background\":\"#007bff\",\"icon\":{\"className\":\"fas fa-info-circle\",\"color\":\"white\",\"tagName\":\"i\"},\"type\":\"info\"}],\"kind\":null,\"name\":\"types\"}]},{\"extends\":null,\"module\":null,\"name\":\"Notification\",\"overrides\":[],\"properties\":[{\"default\":null,\"kind\":null,\"name\":\"background\"},{\"default\":3000,\"kind\":null,\"name\":\"duration\"},{\"default\":null,\"kind\":null,\"name\":\"icon\"},{\"default\":\"\",\"kind\":null,\"name\":\"message\"},{\"default\":null,\"kind\":null,\"name\":\"notification_type\"},{\"default\":false,\"kind\":null,\"name\":\"_destroyed\"}]},{\"extends\":null,\"module\":null,\"name\":\"TemplateActions1\",\"overrides\":[],\"properties\":[{\"default\":0,\"kind\":null,\"name\":\"open_modal\"},{\"default\":0,\"kind\":null,\"name\":\"close_modal\"}]},{\"extends\":null,\"module\":null,\"name\":\"MaterialTemplateActions1\",\"overrides\":[],\"properties\":[{\"default\":0,\"kind\":null,\"name\":\"open_modal\"},{\"default\":0,\"kind\":null,\"name\":\"close_modal\"}]}],\"roots\":{\"references\":[{\"attributes\":{\"num_minor_ticks\":5,\"tickers\":[{\"id\":\"1908\"},{\"id\":\"1909\"},{\"id\":\"1910\"},{\"id\":\"1911\"},{\"id\":\"1912\"},{\"id\":\"1913\"},{\"id\":\"1914\"},{\"id\":\"1915\"},{\"id\":\"1916\"},{\"id\":\"1917\"},{\"id\":\"1918\"},{\"id\":\"1919\"}]},\"id\":\"1864\",\"type\":\"DatetimeTicker\"},{\"attributes\":{\"source\":{\"id\":\"1890\"}},\"id\":\"1897\",\"type\":\"CDSView\"},{\"attributes\":{\"axis\":{\"id\":\"1867\"},\"coordinates\":null,\"dimension\":1,\"grid_line_color\":null,\"group\":null,\"ticker\":null},\"id\":\"1870\",\"type\":\"Grid\"},{\"attributes\":{\"mantissas\":[1,2,5],\"max_interval\":500.0,\"num_minor_ticks\":0},\"id\":\"1908\",\"type\":\"AdaptiveTicker\"},{\"attributes\":{\"coordinates\":null,\"data_source\":{\"id\":\"1890\"},\"glyph\":{\"id\":\"1893\"},\"group\":null,\"hover_glyph\":null,\"muted_glyph\":{\"id\":\"1895\"},\"nonselection_glyph\":{\"id\":\"1894\"},\"selection_glyph\":{\"id\":\"1922\"},\"view\":{\"id\":\"1897\"}},\"id\":\"1896\",\"type\":\"GlyphRenderer\"},{\"attributes\":{},\"id\":\"1905\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"line_color\":\"#fc4f30\",\"line_width\":2,\"x\":{\"field\":\"index\"},\"y\":{\"field\":\"value\"}},\"id\":\"1926\",\"type\":\"Line\"},{\"attributes\":{},\"id\":\"1873\",\"type\":\"WheelZoomTool\"},{\"attributes\":{\"base\":60,\"mantissas\":[1,2,5,10,15,20,30],\"max_interval\":1800000.0,\"min_interval\":1000.0,\"num_minor_ticks\":0},\"id\":\"1909\",\"type\":\"AdaptiveTicker\"},{\"attributes\":{},\"id\":\"1871\",\"type\":\"SaveTool\"},{\"attributes\":{\"line_color\":\"#30a2da\",\"line_width\":2,\"x\":{\"field\":\"index\"},\"y\":{\"field\":\"value\"}},\"id\":\"1922\",\"type\":\"Line\"},{\"attributes\":{\"source\":{\"id\":\"1923\"}},\"id\":\"1930\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"1924\",\"type\":\"Selection\"},{\"attributes\":{\"base\":24,\"mantissas\":[1,2,4,6,8,12],\"max_interval\":43200000.0,\"min_interval\":3600000.0,\"num_minor_ticks\":0},\"id\":\"1910\",\"type\":\"AdaptiveTicker\"},{\"attributes\":{\"overlay\":{\"id\":\"1876\"}},\"id\":\"1874\",\"type\":\"BoxZoomTool\"},{\"attributes\":{},\"id\":\"1919\",\"type\":\"YearsTicker\"},{\"attributes\":{\"line_color\":\"#fc4f30\",\"line_width\":2,\"x\":{\"field\":\"index\"},\"y\":{\"field\":\"value\"}},\"id\":\"1956\",\"type\":\"Line\"},{\"attributes\":{\"line_alpha\":0.1,\"line_color\":\"#fc4f30\",\"line_width\":2,\"x\":{\"field\":\"index\"},\"y\":{\"field\":\"value\"}},\"id\":\"1927\",\"type\":\"Line\"},{\"attributes\":{},\"id\":\"1875\",\"type\":\"ResetTool\"},{\"attributes\":{\"days\":[1,15]},\"id\":\"1914\",\"type\":\"DaysTicker\"},{\"attributes\":{\"label\":{\"value\":\"Predicted\"},\"renderers\":[{\"id\":\"1929\"}]},\"id\":\"1955\",\"type\":\"LegendItem\"},{\"attributes\":{\"months\":[0,1,2,3,4,5,6,7,8,9,10,11]},\"id\":\"1915\",\"type\":\"MonthsTicker\"},{\"attributes\":{\"click_policy\":\"mute\",\"coordinates\":null,\"group\":null,\"items\":[{\"id\":\"1921\"},{\"id\":\"1955\"}],\"location\":[0,0],\"title\":\"Variable\"},\"id\":\"1920\",\"type\":\"Legend\"},{\"attributes\":{\"end\":1564358400000.0,\"reset_end\":1564358400000.0,\"reset_start\":1550620800000.0,\"start\":1550620800000.0,\"tags\":[[[\"index\",\"index\",null]]]},\"id\":\"1850\",\"type\":\"Range1d\"},{\"attributes\":{\"coordinates\":null,\"data_source\":{\"id\":\"1923\"},\"glyph\":{\"id\":\"1926\"},\"group\":null,\"hover_glyph\":null,\"muted_glyph\":{\"id\":\"1928\"},\"nonselection_glyph\":{\"id\":\"1927\"},\"selection_glyph\":{\"id\":\"1956\"},\"view\":{\"id\":\"1930\"}},\"id\":\"1929\",\"type\":\"GlyphRenderer\"},{\"attributes\":{},\"id\":\"1889\",\"type\":\"AllLabels\"},{\"attributes\":{\"bottom_units\":\"screen\",\"coordinates\":null,\"fill_alpha\":0.5,\"fill_color\":\"lightgrey\",\"group\":null,\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":1.0,\"line_color\":\"black\",\"line_dash\":[4,4],\"line_width\":2,\"right_units\":\"screen\",\"syncable\":false,\"top_units\":\"screen\"},\"id\":\"1876\",\"type\":\"BoxAnnotation\"},{\"attributes\":{\"days\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31]},\"id\":\"1911\",\"type\":\"DaysTicker\"},{\"attributes\":{\"days\":[1,4,7,10,13,16,19,22,25,28]},\"id\":\"1912\",\"type\":\"DaysTicker\"},{\"attributes\":{\"line_alpha\":0.2,\"line_color\":\"#fc4f30\",\"line_width\":2,\"x\":{\"field\":\"index\"},\"y\":{\"field\":\"value\"}},\"id\":\"1928\",\"type\":\"Line\"},{\"attributes\":{\"days\":[1,8,15,22]},\"id\":\"1913\",\"type\":\"DaysTicker\"},{\"attributes\":{\"months\":[0,6]},\"id\":\"1918\",\"type\":\"MonthsTicker\"},{\"attributes\":{},\"id\":\"1886\",\"type\":\"AllLabels\"},{\"attributes\":{\"months\":[0,2,4,6,8,10]},\"id\":\"1916\",\"type\":\"MonthsTicker\"},{\"attributes\":{\"callback\":null,\"formatters\":{\"@{index}\":\"datetime\"},\"renderers\":[{\"id\":\"1896\"},{\"id\":\"1929\"}],\"tags\":[\"hv_created\"],\"tooltips\":[[\"Variable\",\"@{Variable}\"],[\"index\",\"@{index}{%F %T}\"],[\"value\",\"@{value}\"]]},\"id\":\"1852\",\"type\":\"HoverTool\"},{\"attributes\":{},\"id\":\"1888\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"months\":[0,4,8]},\"id\":\"1917\",\"type\":\"MonthsTicker\"},{\"attributes\":{\"label\":{\"value\":\"Real\"},\"renderers\":[{\"id\":\"1896\"}]},\"id\":\"1921\",\"type\":\"LegendItem\"},{\"attributes\":{},\"id\":\"1940\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"line_color\":\"#30a2da\",\"line_width\":2,\"x\":{\"field\":\"index\"},\"y\":{\"field\":\"value\"}},\"id\":\"1893\",\"type\":\"Line\"},{\"attributes\":{\"end\":13881.3572993,\"reset_end\":13881.3572993,\"reset_start\":2264.4329837,\"start\":2264.4329837,\"tags\":[[[\"value\",\"value\",null]]]},\"id\":\"1851\",\"type\":\"Range1d\"},{\"attributes\":{\"line_alpha\":0.1,\"line_color\":\"#30a2da\",\"line_width\":2,\"x\":{\"field\":\"index\"},\"y\":{\"field\":\"value\"}},\"id\":\"1894\",\"type\":\"Line\"},{\"attributes\":{},\"id\":\"1872\",\"type\":\"PanTool\"},{\"attributes\":{\"line_alpha\":0.2,\"line_color\":\"#30a2da\",\"line_width\":2,\"x\":{\"field\":\"index\"},\"y\":{\"field\":\"value\"}},\"id\":\"1895\",\"type\":\"Line\"},{\"attributes\":{\"data\":{\"Variable\":[\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\",\"Predicted\"],\"index\":{\"__ndarray__\":\"AABAO4OQdkIAAACh1ZB2QgAAwAYokXZCAACAbHqRdkIAAEDSzJF2QgAAADgfknZCAADAnXGSdkIAAIADxJJ2QgAAQGkWk3ZCAAAAz2iTdkIAAMA0u5N2QgAAgJoNlHZCAABAAGCUdkIAAABmspR2QgAAwMsElXZCAACAMVeVdkIAAECXqZV2QgAAAP37lXZCAADAYk6WdkIAAIDIoJZ2QgAAQC7zlnZCAAAAlEWXdkIAAMD5l5d2QgAAgF/ql3ZCAABAxTyYdkIAAAArj5h2QgAAwJDhmHZCAACA9jOZdkIAAEBchpl2QgAAAMLYmXZCAADAJyuadkIAAICNfZp2QgAAQPPPmnZCAAAAWSKbdkIAAMC+dJt2QgAAgCTHm3ZCAABAihmcdkIAAADwa5x2QgAAwFW+nHZCAACAuxCddkIAAEAhY512QgAAAIe1nXZCAADA7AeedkIAAIBSWp52QgAAQLisnnZCAAAAHv+edkIAAMCDUZ92QgAAgOmjn3ZCAABAT/afdkIAAAC1SKB2QgAAwBqboHZCAACAgO2gdkIAAEDmP6F2QgAAAEySoXZCAADAseShdkIAAIAXN6J2QgAAQH2JonZCAAAA49uidkIAAMBILqN2QgAAgK6Ao3ZCAABAFNOjdkIAAAB6JaR2QgAAwN93pHZCAACARcqkdkIAAECrHKV2QgAAABFvpXZCAADAdsGldkIAAIDcE6Z2QgAAQEJmpnZCAAAAqLimdkIAAMANC6d2QgAAgHNdp3ZCAABA2a+ndkIAAAA/Aqh2QgAAwKRUqHZCAACACqeodkIAAEBw+ah2QgAAANZLqXZCAADAO56pdkIAAICh8Kl2QgAAQAdDqnZCAAAAbZWqdkIAAMDS56p2QgAAgDg6q3ZCAABAnoyrdkIAAAAE36t2QgAAwGkxrHZCAACAz4OsdkIAAEA11qx2QgAAAJsorXZCAADAAHutdkIAAIBmza12QgAAQMwfrnZCAAAAMnKudkIAAMCXxK52QgAAgP0Wr3ZCAABAY2mvdkIAAADJu692QgAAwC4OsHZCAACAlGCwdkIAAED6srB2QgAAAGAFsXZCAADAxVexdkIAAIArqrF2QgAAQJH8sXZCAAAA906ydkIAAMBcobJ2QgAAgMLzsnZCAABAKEazdkIAAACOmLN2QgAAwPPqs3ZCAACAWT20dkIAAEC/j7R2QgAAACXitHZCAADAijS1dkIAAIDwhrV2QgAAQFbZtXZCAAAAvCu2dkIAAMAhfrZ2QgAAgIfQtnZCAABA7SK3dkIAAABTdbd2QgAAwLjHt3ZCAACAHhq4dkIAAECEbLh2QgAAAOq+uHZCAADATxG5dkIAAIC1Y7l2QgAAQBu2uXZCAAAAgQi6dkIAAMDmWrp2QgAAgEytunZCAABAsv+6dkIAAAAYUrt2QgAAwH2ku3ZCAACA4/a7dkIAAEBJSbx2QgAAAK+bvHZCAADAFO68dkIAAIB6QL12QgAAQOCSvXZCAAAARuW9dkIAAMCrN752QgAAgBGKvnZCAABAd9y+dkIAAADdLr92QgAAwEKBv3ZCAACAqNO/dkIAAEAOJsB2QgAAAHR4wHZCAADA2crAdkIAAIA/HcF2QgAAQKVvwXZCAAAAC8LBdkIAAMBwFMJ2QgAAgNZmwnZCAABAPLnCdkIAAACiC8N2QgAAwAdew3ZCAACAbbDDdkI=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[160]},\"value\":{\"__ndarray__\":\"+snJRexZvkWcMNVFzTjBRdzGwkWrj8RF9N2zRSUtuEW9iL9F9wzLRQkRxEXmVrxFRavDRdeM2UXBfCBG3yHBRde/xkU4Yr1FhKLbRSPVyEW/e8ZFqm/JReN9xkVm3MJFMFAhRk4n0UVVvM9FPUPNRVEg1UVrYcpFhBjBRWUnx0VOAsNFMfjGRap8yEUfTMxFM8a+RSeUxEWWlBNGImrFRfjhwkX6OdNF8b33RZaG30Wf0fRFKxHERTtoyEWR3rpFiV/SRZ/ce0U/RshFEKnRRcKwvEXSecJFkvPJRaTEwkVJAb9FW43DRYQ4z0URyY5Fzv7URUPye0VH7MJFOsfLRU4n4EWgGQtGW1e6RdjOwUXD0cFF0wm9RZK6F0Yaw8dFSwzLRfcv2UW5vtBFc9PXRd2K0EWSitBFMaTGRRbq3EUoXMdFY1snRsw7G0aARvZFqQHIRWkhmEXXvshF6rXCRSflu0V/vbxFAty9RernEUaLI8NF4UHMRUQzyUW8k8dF+cgnRnWpy0VPYM5Foz7ERRAswUUYYtZF23HgRW54GUayksxF5WjFRQJSv0VzS+BFN77CRc9ssEUDyMpFckHGRSVQ4EUqMwxGSh+aRXOiw0UBxhhG7YLkRX5eHkbp7dlFsTDTRbIAwEV7Lt1Fqw7WRciHwUVPRb1FGBkWRrY6xUVMsr1FaUwTRhyq0kUWNcBFapvMRVohwUVnzcpFyKIgRgzjy0Wo+sJFO8XKRUaOx0UXictFNdC+Ravh5EXQsMNF8lzHRe4Sy0X/gsFF4ZvARZSF0UUknL5FqgbrReFZxEVlE85FtgTURSxnGEY+VLpFSBvrRSZLxkXzwslFe/3DRQ==\",\"dtype\":\"float32\",\"order\":\"little\",\"shape\":[160]}},\"selected\":{\"id\":\"1924\"},\"selection_policy\":{\"id\":\"1940\"}},\"id\":\"1923\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"1891\",\"type\":\"Selection\"},{\"attributes\":{\"margin\":[5,5,5,5],\"name\":\"HSpacer02699\",\"sizing_mode\":\"stretch_width\"},\"id\":\"1849\",\"type\":\"Spacer\"},{\"attributes\":{\"tools\":[{\"id\":\"1852\"},{\"id\":\"1871\"},{\"id\":\"1872\"},{\"id\":\"1873\"},{\"id\":\"1874\"},{\"id\":\"1875\"}]},\"id\":\"1877\",\"type\":\"Toolbar\"},{\"attributes\":{\"below\":[{\"id\":\"1863\"}],\"center\":[{\"id\":\"1866\"},{\"id\":\"1870\"}],\"height\":300,\"left\":[{\"id\":\"1867\"}],\"margin\":[5,5,5,5],\"min_border_bottom\":10,\"min_border_left\":10,\"min_border_right\":10,\"min_border_top\":10,\"renderers\":[{\"id\":\"1896\"},{\"id\":\"1929\"}],\"right\":[{\"id\":\"1920\"}],\"sizing_mode\":\"fixed\",\"title\":{\"id\":\"1855\"},\"toolbar\":{\"id\":\"1877\"},\"width\":700,\"x_range\":{\"id\":\"1850\"},\"x_scale\":{\"id\":\"1859\"},\"y_range\":{\"id\":\"1851\"},\"y_scale\":{\"id\":\"1861\"}},\"id\":\"1854\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{\"data\":{\"Variable\":[\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\",\"Real\"],\"index\":{\"__ndarray__\":\"AABAO4OQdkIAAACh1ZB2QgAAwAYokXZCAACAbHqRdkIAAEDSzJF2QgAAADgfknZCAADAnXGSdkIAAIADxJJ2QgAAQGkWk3ZCAAAAz2iTdkIAAMA0u5N2QgAAgJoNlHZCAABAAGCUdkIAAABmspR2QgAAwMsElXZCAACAMVeVdkIAAECXqZV2QgAAAP37lXZCAADAYk6WdkIAAIDIoJZ2QgAAQC7zlnZCAAAAlEWXdkIAAMD5l5d2QgAAgF/ql3ZCAABAxTyYdkIAAAArj5h2QgAAwJDhmHZCAACA9jOZdkIAAEBchpl2QgAAAMLYmXZCAADAJyuadkIAAICNfZp2QgAAQPPPmnZCAAAAWSKbdkIAAMC+dJt2QgAAgCTHm3ZCAABAihmcdkIAAADwa5x2QgAAwFW+nHZCAACAuxCddkIAAEAhY512QgAAAIe1nXZCAADA7AeedkIAAIBSWp52QgAAQLisnnZCAAAAHv+edkIAAMCDUZ92QgAAgOmjn3ZCAABAT/afdkIAAAC1SKB2QgAAwBqboHZCAACAgO2gdkIAAEDmP6F2QgAAAEySoXZCAADAseShdkIAAIAXN6J2QgAAQH2JonZCAAAA49uidkIAAMBILqN2QgAAgK6Ao3ZCAABAFNOjdkIAAAB6JaR2QgAAwN93pHZCAACARcqkdkIAAECrHKV2QgAAABFvpXZCAADAdsGldkIAAIDcE6Z2QgAAQEJmpnZCAAAAqLimdkIAAMANC6d2QgAAgHNdp3ZCAABA2a+ndkIAAAA/Aqh2QgAAwKRUqHZCAACACqeodkIAAEBw+ah2QgAAANZLqXZCAADAO56pdkIAAICh8Kl2QgAAQAdDqnZCAAAAbZWqdkIAAMDS56p2QgAAgDg6q3ZCAABAnoyrdkIAAAAE36t2QgAAwGkxrHZCAACAz4OsdkIAAEA11qx2QgAAAJsorXZCAADAAHutdkIAAIBmza12QgAAQMwfrnZCAAAAMnKudkIAAMCXxK52QgAAgP0Wr3ZCAABAY2mvdkIAAADJu692QgAAwC4OsHZCAACAlGCwdkIAAED6srB2QgAAAGAFsXZCAADAxVexdkIAAIArqrF2QgAAQJH8sXZCAAAA906ydkIAAMBcobJ2QgAAgMLzsnZCAABAKEazdkIAAACOmLN2QgAAwPPqs3ZCAACAWT20dkIAAEC/j7R2QgAAACXitHZCAADAijS1dkIAAIDwhrV2QgAAQFbZtXZCAAAAvCu2dkIAAMAhfrZ2QgAAgIfQtnZCAABA7SK3dkIAAABTdbd2QgAAwLjHt3ZCAACAHhq4dkIAAECEbLh2QgAAAOq+uHZCAADATxG5dkIAAIC1Y7l2QgAAQBu2uXZCAAAAgQi6dkIAAMDmWrp2QgAAgEytunZCAABAsv+6dkIAAAAYUrt2QgAAwH2ku3ZCAACA4/a7dkIAAEBJSbx2QgAAAK+bvHZCAADAFO68dkIAAIB6QL12QgAAQOCSvXZCAAAARuW9dkIAAMCrN752QgAAgBGKvnZCAABAd9y+dkIAAADdLr92QgAAwEKBv3ZCAACAqNO/dkIAAEAOJsB2QgAAAHR4wHZCAADA2crAdkIAAIA/HcF2QgAAQKVvwXZCAAAAC8LBdkIAAMBwFMJ2QgAAgNZmwnZCAABAPLnCdkIAAACiC8N2QgAAwAdew3ZCAACAbbDDdkI=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[160]},\"value\":{\"__ndarray__\":\"08H6PzNPvUAtPgXADLe5QP////+/qbRAvjEEABdytECy2vy/3ui5QLwi+N86U7lAKy/5n3BmuUCdSgaAQu+1QJ1KBoDCxbpAIef9fxSQwECR8/4/yue5QLwi+N+6hbhAkfP+P8qzuUCFqwMgLgi5QAAAAACAxL5A1dAGYI9dukAf2PFfOFWuQIarAyAuka1AY7X5f70rrEDpYP2fB7LAQNPB+j9zy75Astr8v17nukCbO/pf5mm5QOEnDqBHcK9AeVT83yNpxkB7VPzfERbDQGO1+X+9JblAGJ8CYHijt0C7Ivjfem20QGXEBaCZxrRA4CcOoMdGrUBNJQNAId64QE4lA0Ch8b5AkPP+P4pWwUAXnwJguHy1QGXEBaDZorhAvCL43/rdrUAXnwJguFi5QCHn/X8UYrxALT4FwIxqvUDpYP2f2QzAQNTQBmAPLrlACkj7H1x+vkBE3QcgBQuuQOlg/Z/HSsFAIef9f5ROtUCGqwMgrjvBQHpU/N+R/bdAZcQFoBkMr0AKSPsf3C+wQELO+/+o+rhAeVT832MMwED2twTgY8i3QKmhDcAeqq1AelT831HtvkDKiAtAsymvQB/Y8V84HK5ACkj7H1wXvED2twTgo+a0QETdByAFQalAAAAAAOCYw0BXXvI/YbirQJ1KBoDCVrtAyHn/H0UgwkDIef8fhQjDQFlt/l/vtcBA6WD9n8chu0DfGAKAaxarQN0J9l8PmKxAeUXwv/Wbr0BZbf5frwrHQN8YAoBrjMBAmzv6X2aqrEDfGAKAXebDQJ1KBoDC061AelT839FCrEAAAAAAgJGvQBefAmA4L8RA08H6P7MuukAj9gmgcAmsQIarAyDcc8FAelT836M4yUCy2vy/LMfEQJ1KBoBCdLRAF58CYGZzwUC8IvjfepqvQOEnDqBHyqxAbwwBwLUFuUD2twTgY4K7QIarAyBuprhATiUDQBPbwUBvDAHA9cu8QIarAyAcpMFATiUDQBM/w0DdCfZfjxGuQOEnDqDH/atA6WD9n4chyEAKSPsfHKW4QETdByAFyrlA3xgCgGtOr0BE3Qcgha65QAAAAAAAJr5A3Qn2Xw8fr0AWnwJguGfCQL4xBADXraxA3xgCgCtGuUC8IvjfOsK8QL4xBADJFcBALT4FwAxmvECdSgaAQk+vQLwi+N96qK5AWW3+X+8qxEC8IvjfOrezQMh5/x/F4MRAvjEEAFdTr0Af2PFfuPetQE4lA0AT8MJAOIYA4JqsxUCR8/4/Sj6/QDiGAOD6O8NATSUDQCEeukAuPgXAzDu4QFde8j/hUq5AF58CYHhttkBOJQNAIRawQJHz/j8Kh79ATiUDQCExxUCaO/pf5n24QHpU/N8R17pACkj7H1x/vkAAAAAAgA+2QN4YAoDrsrhAnDv6X+a6vUDV0AZgD7u8QHhF8L/17q1AhqsDIO4Lv0AKSPsfXPq4QAIPDCDcOa9AWW3+X69YxUDdCfZfjyOtQLLa/L9soMJAY7X5fz3zqkC+MQQAFzWwQETdByCFiKxA6WD9n7k7w0AtPgXATO24QE4lA0DzOsNAsdr8v57Cu0ABAAAAQAG/QCDn/X/0G8BA/////3/Zs0BvDAHAddbCQIarAyCuoK9AvjEEAOllwECy2vy//jPFQLwi+N96SqxA3xgCgCt0tkArL/mfsBCwQELO+//oqbhAIef9f5RtuUA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[160]}},\"selected\":{\"id\":\"1891\"},\"selection_policy\":{\"id\":\"1905\"}},\"id\":\"1890\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"axis\":{\"id\":\"1863\"},\"coordinates\":null,\"grid_line_color\":null,\"group\":null,\"ticker\":null},\"id\":\"1866\",\"type\":\"Grid\"},{\"attributes\":{\"coordinates\":null,\"group\":null,\"text_color\":\"black\",\"text_font_size\":\"12pt\"},\"id\":\"1855\",\"type\":\"Title\"},{\"attributes\":{},\"id\":\"1868\",\"type\":\"BasicTicker\"},{\"attributes\":{\"axis_label\":\"Price\",\"coordinates\":null,\"formatter\":{\"id\":\"1888\"},\"group\":null,\"major_label_policy\":{\"id\":\"1889\"},\"ticker\":{\"id\":\"1868\"}},\"id\":\"1867\",\"type\":\"LinearAxis\"},{\"attributes\":{},\"id\":\"1885\",\"type\":\"DatetimeTickFormatter\"},{\"attributes\":{\"margin\":[5,5,5,5],\"name\":\"HSpacer02700\",\"sizing_mode\":\"stretch_width\"},\"id\":\"2149\",\"type\":\"Spacer\"},{\"attributes\":{},\"id\":\"1859\",\"type\":\"LinearScale\"},{\"attributes\":{\"children\":[{\"id\":\"1849\"},{\"id\":\"1854\"},{\"id\":\"2149\"}],\"margin\":[0,0,0,0],\"name\":\"Row02695\",\"tags\":[\"embedded\"]},\"id\":\"1848\",\"type\":\"Row\"},{\"attributes\":{\"axis_label\":\"Date\",\"coordinates\":null,\"formatter\":{\"id\":\"1885\"},\"group\":null,\"major_label_policy\":{\"id\":\"1886\"},\"ticker\":{\"id\":\"1864\"}},\"id\":\"1863\",\"type\":\"DatetimeAxis\"},{\"attributes\":{},\"id\":\"1861\",\"type\":\"LinearScale\"}],\"root_ids\":[\"1848\"]},\"title\":\"Bokeh Application\",\"version\":\"2.4.2\"}};\n",
       "    var render_items = [{\"docid\":\"7766cbb3-89d3-4550-a815-f7f483bbe3c8\",\"root_ids\":[\"1848\"],\"roots\":{\"1848\":\"b947f30b-82fc-4719-bf2b-c2a430de7c19\"}}];\n",
       "    root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "  }\n",
       "  if (root.Bokeh !== undefined && root.Bokeh.Panel !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined && root.Bokeh.Panel !== undefined) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else if (document.readyState == \"complete\") {\n",
       "        attempts++;\n",
       "        if (attempts > 200) {\n",
       "          clearInterval(timer);\n",
       "          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "        }\n",
       "      }\n",
       "    }, 25, root)\n",
       "  }\n",
       "})(window);</script>"
      ],
      "text/plain": [
       ":NdOverlay   [Variable]\n",
       "   :Curve   [index]   (value)"
      ]
     },
     "execution_count": 47,
     "metadata": {
      "application/vnd.holoviews_exec.v0+json": {
       "id": "1848"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plot the real vs predicted values as a line chart\n",
    "stocks.hvplot.line(xlabel=\"Date\", ylabel=\"Price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "vscode": {
   "interpreter": {
    "hash": "f7002163cacfad24393b7620eed21e926662f31a9c25cb71446747eea29a57e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
